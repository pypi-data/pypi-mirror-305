% TeX Source
%
% Author: Tetsuya Ishikawa <tiskw111gmail.com>

\documentclass[twocolumn,a4paper,10pt]{article}

\usepackage{tiskwdoc}

\begin{document}

\title{Random Fourier Features for Gaussian Process Model}
\author{Tetsuya Ishikawa \\ \normalsize\texttt{tiskw111@gmail.com}}
\maketitle
\thispagestyle{fancy}

\section*{Abstract}\titlebar
% {{{

This article describes the procedure for applying random Fourier features~\cite{Rahimi2007}
to Gaussian process model~\cite{Rasmussen2006}. This makes it possible to speed up the training
and inference of Gaussian process model, and to apply the model to large-scale data.

Gaussian process model~\cite{Rasmussen2006} is one of the supervised machine learning frameworks
designed on a probability space, and is widely used for regression and classification tasks, as
well as support vector machine and random forest. The major difference between Gaussian process
model and other machine learning models is that Gaussian process model is a \textit{stochastic}
model. In other words, since Gaussian process model is formulated as a stochastic model,
it can provide not only the predicted value but also a measure of uncertainty for the prediction.
This is a very useful property that can improve the explainability of machine learning model.

On the other hand, Gaussian process model is also known for its high computational cost of training
and inference. If the total number of training data is $N \in \mathbb{Z}^{+}$, the computational
cost required for training is $O(N^3)$, and the computational cost required for inference is
$O(N^2)$, where $O$ is \textit{Bachmannâ€“Landau notation}. The problem is that the computational cost
is given by a power of the total number of training data $N$, which can be an obstacle when applying
the model to large-scale data. This comes from the fact that Gaussian process model has the same
mathematical structure as kernel methods, in other words, the kernel support vector machine also
has the same problem.

One of the methods to speed up kernel methods is random Fourier features~\cite{Rahimi2007}
(hereinafter abbreviated as RFF). This method can significantly reduce the computational cost while
keeping the flexibility of kernel methods by approximating a kernel function as an inner product
of finite dimensional vectors. Specifically, the computational cost required for training can be
reduced to $O(N D^2)$, and the amount of calculation required for inference can be reduced to
$O(D^2)$, where $D \in \mathbb{Z}^{+}$ is a hyperparameter of RFF and can be specified
independently of the total number of training data $N$. Since Gaussian process model has the same
mathematical structure as kernel methods, RFF can be applied to Gaussian process model as well.
This evolves Gaussian process model into a more powerful, easy-to-use, and highly reliable ML tool.

However, when applying RFF to Gaussian process model, some mathematical techniques are required that
are not straightforward. Unfortunately, there seems to be no article in the world that mentions its
difficulties and solutions, so I decided to leave this document. If you prefer the Japanese version
of this document, see the repository \cite{Ishikawa2021}.

% }}}

\section{Gaussian Process Model Revisited}\titlebar
% {{{

This section gives an overview of Gaussian process model. Unfortunately, this section
can not cover details such as the formulation and derivation of Gaussian process models
due to the limitation of pulp, so if you are interested in the details,
please refer \cite{Rasmussen2006}.

Let $\mathcal{D} = \{(\bs{x}_n, y_n)\}_{n=1}^{N}$ be training data, and $\sigma \in \mathbb{R}^{+}$
be a standard deviation of the label observation error, where $\bs{x}_n \in \mathbb{R}^M$, $y_n \in \mathbb{R}$.
Gaussian process model describes the prediction as a probability variable that follows the normal distribution.
If the test date is $\bs{\xi} \in \mathbb{R}^M$, the expectation of the prediction is given by:
\begin{equation}
    m (\bs{\xi}) = \widehat{m} (\bs{\xi}) + \left( \bs{y} - \widehat{\bs{m}} \right)\tran
    \left (\bs{K} + \sigma^2 \bs{I} \right)^{-1} \bs{k} (\bs {\xi}),
    \label{eqn:gp_exp} \\
\end{equation}
and the covariance of the test data $\bs{\xi}_1, \bs{\xi}_2 \in \mathbb{R}^M$ is given by:
\begin{equation}
    v (\bs{\xi}_1, \bs{\xi}_2) = k (\bs{\xi}_1, \bs{\xi}_2)
    - \bs{k} (\bs{\xi}_1)\tran \left( \bs{K} + \sigma^2 \bs{I} \right)^{-1} \bs{k} (\bs{\xi}_2),
    \label{eqn:gp_cov}
\end{equation}
where the function $k: \mathbb{R}^M \times \mathbb{R}^M \to \mathbb{R}$ is a kernel function,
the matrix $\bs{K} \in \mathbb{R}^{N \times N}$ is a kernel matrix defined as
\begin {equation}
    \bs{K} = \begin{pmatrix}
        k (\bs{x}_1, \bs{x}_1) & \cdots & k (\bs{x}_1, \bs{x}_N) \\
        \vdots & \ddots & \vdots \\
        k (\bs{x}_N, \bs{x}_1) & \cdots & k (\bs{x}_N, \bs{x}_N) \\
    \end{pmatrix},
    \label{eqn:def_kernel_matrix}
\end{equation}
and the vector function $\bs{k} (\bs {\xi}): \mathbb{R}^M \to \mathbb{R}^N$ and
the vector $\bs{y} \in \mathbb{R}^N$ is defined as
\begin{equation}
    \bs{k} (\bs{\xi}) = \begin{pmatrix}
        k (\bs{\xi}, \bs{x}_1) \\
        \vdots \\
        k (\bs{\xi}, \bs{x}_N) \\
    \end{pmatrix},
    \hspace{10pt}
    \bs{y} = \begin{pmatrix}
        y_1 \\ \vdots \\ y_N \\
    \end{pmatrix},
\end{equation}
respectively.
Also, $\widehat{m} (\bs{\xi})$ is the prior distribution of the prediction, and
$\widehat{\bs{m}} = (\widehat{m} (\bs{x}_1), \ldots, \widehat{m} (\bs{x}_N))\tran$ is
the prior distribution of the predicted values of the training data. If you don't need to set
prior distribution, it's common to set $\widehat{m} (\cdot) = 0$ and $\widehat{\bs{m}} = \bs{0}$.

You can compute the variance of the prediction of the test data $\bs{\xi}$
by substituting $\bs{\xi}_1 = \bs{\xi}_2 = \bs{\xi}$ into the equation (\ref{eqn:gp_cov}),
\begin{equation}
    v (\bs{\xi}, \bs{\xi}) = k (\bs{\xi}, \bs{\xi})
    - \bs{k} (\bs{\xi})\tran \left( \bs{K} + \sigma^2 \bs{I} \right)^{-1} \bs{k} (\bs{\xi}).
\end{equation}

% }}}

\section{RFF Revisited}\titlebar
% {{{

In this section, we revisit random Fourier features. Unfortunately, this article don't have
enough space to explain the details as the same as the previous section, therefore if you
would like to know more details, please refer to the original paper \cite{Rahimi2007}.

Let the function $k: \mathbb{R}^M \times \mathbb{R}^M \to \mathbb{R}$ be a kernel function.
In RFF, the kernel function can be approximated as
\begin{equation}
    k (\bs{x}_1, \bs{x}_2) \simeq \bs{\phi}(\bs{x}_1)\tran \bs{\phi}(\bs{x}_2),
    \label{eqn:rff_kernel_approx}
\end{equation}
where $\bs{\phi}(\bs{x}) \in \mathbb{R}^D$ is a feature vector extracted from 
the data $\bs{x}$ and $D \in \mathbb{Z}^+$ is the dimension of $\bs{\phi}(\bs{x})$.
Note that the approximation (\ref{eqn:rff_kernel_approx}) is equivalent with
\begin{equation}
    \bs{K} \simeq \bs{\Phi}\tran \bs{\Phi},
    \label{eqn:rff_kernel_approx_matrix}
\end{equation}
where $\bs{K}$ is defined as the equation (\ref{eqn:def_kernel_matrix}), and $\bs{\Phi}$ is defined
as $\bs{\Phi} = (\bs{\phi} (\bs{x}_1), \ldots, \bs{\phi} (\bs{x}_N))$.
The larger the dimension $D$, the higher the approximation accuracy of the equation
(\ref{eqn:rff_kernel_approx}), while the larger the dimension $D$, the greater compurational cost.

The actual function form of the feature vector $\bs{\phi}(\bs{x})$ depends on the kernel function.
For example, in the case of the RBF kernel
\begin{equation}
    k (\bs{x}_1, \bs{x}_2) = \exp \left (- \gamma \| \bs{x}_1 - \bs{x}_2 \|^2 \right),
\end{equation}
which is the most famous kernel function, the vector $\bs{\phi} (\bs{x})$ is given by
\begin{equation}
    \bs{\phi} (\bs{x}) = \cos \left( \bs{Wx} + \bs{u} \right),
\end{equation}
where, the matrix $\bs{W} \in \mathbb{R}^{D \times M} $ is a random matrix in which each element
is sampled from the normal distribution $\mathcal{N} (0, \frac{1}{4 \gamma})$:
\begin{equation}
    \bs{W} \sim \mathcal{N} \left( \bs{0}, \frac{1}{4 \gamma} \bs{I} \right),
\end{equation}
and the vector $\bs{u} \in \mathbb{R}^M$ is a random vector sampled from the 
uniform distribution over the range $[0, 2 \pi)$:
\begin{equation}
    \bs{u} \sim \mathcal{U} [0, 2 \pi).
\end{equation}

This approximation method is called random Fourier features because the 
feature extraction vector $\bs{\phi}(\bs{x})$ is derived from the Fourier 
transform of the kernel matrix $\mathcal{F}[k](\omega)$.
Please refer to the original paper \cite{Rahimi2007} for more details.

% }}}

\section{Gaussian Process Model and RFF}\titlebar
% {{{

In this section, we apply RFF to Gaussian process model
and theoretically confirm the effect of speeding up.

\subsection{Computational complexity of Gaussian process model before applying RFF}

First, let's check the computational cost required for training and inferring a normal Gaussian
process model. As a premise, it is assumed that the number of training data $N \in \mathbb{Z}^{+}$
is sufficiently larger than the dimension $D \in \mathbb{Z}^{+}$ which is a hyperparameter of RFF.
Here, the bottleneck of training computational cost is the calculation of the inverse matrix
$\left (\bs{K} + \sigma^2 \bs{I} \right)^{-1}$ in the formulas (\ref{eqn:gp_exp}) and (\ref{eqn:gp_cov}).
Since the size of this matrix is $N \times N$, the computational cost for the training is $O(N^3)$.
Next, the bottleneck of the inference is the matrix multiplication
$\left (\bs{y} -\widehat{\bs{m}} \right)\tran \left( \bs{K} + \sigma^2 \bs{I} \right)^{-1}$
for the expectation prediction and
$\bs{k} (\bs{\xi}_1)\tran \left( \bs{K} - \sigma^2 \bs{I} \right)^{-1} \bs{k} (\bs{\xi}_2)$,
for the covariance prediction whose computational costs is $O(N)$ and $O(N^2)$ respectively.

\subsection{Applying RFF to expectation of prediction}

Now, let's apply RFF to Gaussian process model. First of all, if you substitute the RFF
approximation formula (\ref{eqn:rff_kernel_approx_matrix}) into the formula of expectation of
the prediction in Gaussian process (\ref{eqn:gp_exp}), you'll get
\begin{equation}
    m (\bs{\xi}) = \widehat{m} (\bs{\xi}) + \left( \bs{y} - \widehat{\bs{m}} \right)\tran
    \left( \bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I} \right)^{-1} \bs{\Phi}\tran \bs{\phi} (\bs{\xi}),
    \label{eqn:rffgp_exp_naive}
\end{equation}
where the matrix $\bs{\Phi} \in \mathbb{R}^{D \times N}$ is defined as
$\bs{\Phi} = (\bs{\phi} (\bs{x}_1), \ldots, \bs{\phi} (\bs{x}_N))$.
However, this has not yet speeded up. The complexity bottleneck of the above expression
(\ref{eqn:rffgp_exp_naive}) is still the inverse of the $N \times N$ matrix
$\left( \bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I} \right)^{-1}$.

Now we will add a bit of contrivance to the equation (\ref{eqn:rffgp_exp_naive}).

\vspace*{5pt}
\begin{thmbox}[M]{\bfseries{Lemma 1} (inversion of the matrix $\bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}$)}
    Let $\bs{\Phi} \in \mathbb{R}^{D \times N}$ be a real matrix and $\sigma \in \mathbb{R}^{+}$ be
    a positive real number. Then the matrices $\bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I}_N$ and
    $\bs{\Phi} \bs{\Phi}\tran + \sigma^2 \bs{I}_D$ are regular matrices and the equation
    \begin{equation}
        \left( \bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I}_N \right)^{-1} \bs{\Phi}\tran
        = \bs{\Phi}\tran \left( \bs{\Phi} \bs{\Phi}\tran + \sigma^2 \bs{I}_D \right)^{-1},
    \end{equation}
    holds, where the matrices $\bs{I}_D \in \mathbb{R}^{D \times D}$ and
    $\bs{I}_N \in \mathbb{R}^{N \times N}$ are $D$ and $N$ dimensional identity matrices, respectively.
\end{thmbox}

The proof of the above lemma is given at the end of this article, and let us move on to
the utilization of the lemma to the equation (\ref{eqn:rffgp_exp_naive}).
By the lemma 1, the equation (\ref{eqn:rffgp_exp_naive})
can be transformed as below:
\begin{align}
    m (\bs{\xi})
    &= \widehat{m} (\bs{\xi}) + \left( \bs{y} - \widehat{\bs{m}} \right)\tran
    \left( \bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I} \right)^{-1} \bs{\Phi}\tran
    \bs{\phi} (\bs{\xi}) \notag \\
    &= \widehat{m} (\bs{\xi}) + \left( \bs{y} - \widehat{\bs{m}} \right)\tran
    \bs{\Phi}\tran \left( \bs{\Phi} \bs{\Phi}\tran + \sigma^2 \bs{I} \right)^{-1}
    \bs{\phi} (\bs{\xi}),
    \label{eqn:rffgp_exp}
\end{align}
provided that the two $\bs{I}$'s appearing in the above equation are of different sizes.

Clever readers would have already noticed that the bottleneck has been resolved.
The inverse matrix $\bigl( \bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I} \bigr)^{-1}$,
which was the bottleneck of the expression (\ref{eqn:rffgp_exp_naive}),
became $\bigl( \bs{\Phi} \bs{\Phi}\tran + \sigma^2 \bs{I} \bigr)^{-1}$ in the expressions (\ref{eqn:rffgp_exp})
where the size of the inverse matrix is $D \times D$.
Normally, the dimension $D$ is set sufficiently smaller than the number of training data $N$,
therefore the inverse matrix $\bigl( \bs{\Phi} \bs{\Phi}\tran + \sigma^2 \bs{I} \bigr)^{-1}$
is no longer a bottleneck of computational cost.
The new bottleneck of the expression (\ref{eqn:rffgp_exp}) is the matrix product $\bs{\Phi \Phi}\tran$,
whose computational cost is $O(ND^2)$.
Therefore we've achieved a considerable speedup of the training of Gaussian process model
by applying RFF because the calculational cost before RFF is $O(N^3)$.

\subsection{Applying RFF to covariance of prediction}

Next, we apply RFF to the covariance of the prediction (\ref{eqn:gp_cov}).
By substituting RFF approximation (\ref{eqn:rff_kernel_approx_matrix}) into the formula of
covariance prediction (\ref{eqn:gp_cov}), we obtain
\begin{align}
    v (\bs{\xi}_1, \bs{\xi}_2)
    &= k (\bs{\xi}_1, \bs{\xi}_2) - \bs{k} (\bs{\xi}_1)\tran
    \left( \bs{K} - \sigma^2 \bs{I} \right)^{-1} \bs{k} (\bs{\xi}_2) \notag \\
    &= \bs{\phi}(\bs{\xi}_1)\tran \left\{ \bs{I} - \bs{\Phi}
    \left( \bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I} \right)^{-1}
    \bs{\Phi}\tran \right\} \bs{\phi}(\bs{\xi}_2),
\end{align}
and by applying the lemma 1, we'll get
\begin{align}
    v (\bs{\xi}_1, \bs{\xi}_2)
    &= \bs{\phi}(\bs{\xi}_1)\tran \left\{ \bs{I} - \bs{\Phi} \bs{\Phi}\tran
    \left( \bs{\Phi} \bs{\Phi}\tran + \sigma^2 \bs{I} \right)^{-1} \right\} \bs{\phi}(\bs{\xi}_2).
    \label{eqn:rffgp_cov}
\end{align}
The bottleneck of the expression (\ref{eqn:rffgp_cov}) is, as well as the expectation prediction,
the matrix inverse $\left( \bs{\Phi} \bs{\Phi}\tran + \sigma^2 \bs{I} \right)^{-1}$ is
no longer a bottleneck of the computational cost, and the new bottleneck is the matrix product
$\bs{\Phi \Phi}\tran$ whose calculation cost is $O(ND^2)$.

\subsection{Training and inference process}

The theoretical essence of the application of RFF to Gaussian process was 
mentioned in the previous subsections. In this section, let me review the procedure of
training and inference of Gaussian process model after applying RFF. Each procedure is
described in the algorithms \ref{alg:rffgp_pseudo_code} as pseudo-code,
where $\mathcal{D}$ is defined as $\mathcal{D} = \{(\bs{x}_n, y_n)\}_{n=1}^{N}$,
$\sigma \in \mathbb{R}^+$ is a standard deviation of the measurement error,
and $\bs{\phi}$ is a feature extraction function used in the RFF approximation.

In the algorithm \ref{alg:rffgp_pseudo_code}, the function \texttt{TRAINING\_GP\_WITH\_RFF} is
designed to take the training data $\mathcal{D}$ and the standard deviation $\sigma$ as arguments,
and return a vector $\bs{a}$ and a matrix $\bs{C}$ that will be used in the inference of 
expectation and covariance, respectively. Also, the function \texttt{INFERENCE\_GP\_WITH\_RFF} is
designed to take the inference target data $\bs{\xi}$ and the training result $\bs{a}$ and $\bs{C}$,
and return predicted expectation $\mu$ and predicted variance $\nu$.
In the algorithm \ref{alg:rffgp_pseudo_code}, we computed only the variance of prediction,
however, if you want to compute the covariance of two input data $\bs{\xi}_1$ and $\bs{\xi}_2$,
then please compute $\bs{\phi}(\bs{\xi}_1)\tran \bs{C} \bs{\phi}(\bs{\xi}_2)$.
Also, note that the prior distribution of Gaussian process model is set to 0 for the sake of
simplicity in the algorithm \ref{alg:rffgp_pseudo_code}.

\begin{algorithm}[h]
    \caption{Training of the GP model after RFF}
    \label{alg:rffgp_pseudo_code}
    \begin{algorithmic}[1]
    \Function {training\_gp\_with\_rff}{$\mathcal{D}$, $\sigma$}
        \State $\bs{y} \gets \bigl( y_1, y_2, \ldots, y_N \bigr)\tran$
        \State $\bs{\Phi} \gets \bigl( \bs{\phi}(\bs{x}_1), \bs{\phi}(\bs{x}_1), \ldots, \bs{\phi}(\bs{x}_N) \bigr)$
        \State $\bs{P} \gets \bs{\Phi \Phi}\tran$
        \State $\bs{Q} \gets (\bs{P} + \sigma^2 \bs{I})^{-1}$
        \State $\bs{a} \gets \bs{y}\tran \bs{\Phi}\tran Q$
        \State $\bs{C} \gets \bs{I} - \bs{PQ}$
        \State \Return ($\bs{a}$, $\bs{C}$)
    \EndFunction
    \State
    \Function {inference\_gp\_with\_rff}{$\bs{\xi}$, $\bs{a}$, $\bs{C}$}
        \State $\bs{z} \gets \bs{\phi}(\bs{\xi})$
        \State $\mu \gets \bs{a}\tran \bs{z}$
        \State $v \gets \bs{z}\tran \bs{C} \bs{z}$
        \State \Return ($\mu$, $v$)
    \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection{Computational cost of training and inderence}

Finally, let me summarize the computational cost of the training and inference of Gaussian process
model with random Fourier features. In this section, we assume that the number of training data $N$
is sufficiently larger than the RFF dimension $D$. Also the computational cost of the feature
extraction function $\bs{\phi}$ is ignored because it depends on the kernel function itself.  

As you can see the algorithm \ref{alg:rffgp_pseudo_code}, the bottleneck of the training process is
the matrix multiplication $\bs{\Phi} \bs{\Phi}\tran$ whoes computational cost is $O(N D^2)$.
Note that the matrix inversion $(\bs{P} + \sigma^2 \bs{I})^{-1}$ where
$\bs{P} = \bs{\Phi}\bs{\Phi}\tran$ is not a bottleneck of the training process under
the assumption $N \gg D$, because its computatinal cost is $O(D^3)$.

As for the inference cost, the bottleneck is $\bs{a}\tran \bs{z}$ for the expectation prediction,
and $\bs{z}\tran \bs{C} \bs{z}$ for the covariance prediction that have the 
computational cost $O(D)$ and $O(D^2)$ respectively. The computational cost before and after
applying RFF is summarized in the table \ref{tab:gp_complexity}.

\begin{table}[h]
    \caption{Computational cost of the GP model before/after RFF}
    \label{tab:gp_complexity}
    \begin{center}\begin{tabular}{ccc}
        \hline
         & Training & Inference \\
        \hline
        Before RFF & $O(N^3)$   & $O(N^2)$ \\  
        After RFF  & $O(N D^2)$ & $O(D^2)$ \\
        \hline
    \end{tabular}\end{center}
\end{table}

% }}}

\newpage
\appendix

\section{Appendix 1: Another Approach}\titlebar
% {{{

This section provides another approach to reduce the computational cost of the equation (\ref{eqn:rffgp_exp_naive}).
This approach has almost the same computational cost as the previous approach,
however, the previous approach is a bit more simple and beautiful
\footnote{The previous approach has high similarity with the linear regression, 
however, the approach mentioned in this section doesn't have.}
than this approach. Therefore the readers don't have to pay much attention to this section.

\subsection{Applying RFF to expectation of prediction}

First, let us introduce the \textit{matrix inversion lemma} (it's also referred to as the
\textit{binominal inverse lemma}) which is a useful formula for the expansion of a matrix inverse.  

\vspace*{8pt}
\begin{thmbox}[M]{\bfseries{Lemma 2 (Matrix Inversion Lemma)}}
    Let
    $\bs{A} \in \mathbb{R}^{N \times N}$,
    $\bs{B} \in \mathbb{R}^{N \times M}$,
    $\bs{C} \in \mathbb{R}^{M \times N}$,
    and
    $\bs{D} \in \mathbb{R}^{M \times M}$
    be real matrices. Then the equation
    \begin{equation}
        \left( \bs{A} + \bs{BDC} \right)^{-1} = \bs {A}^{-1} - \bs{A}^{-1} \bs{B}
        \left( \bs{D}^{-1} + \bs{CA}^{-1} \bs{B} \right)^{-1} \bs{CA}^{-1}
        \label{eqn:matrix_inversion_lemma}
    \end{equation}
    holds, where the matrices $\bs{A}$ and $\bs{D}$ are regular matrices.
\end{thmbox}
\vspace*{4pt}

The proof of the matrix inversion lemma is given at the end of this article,
and let us move on to the utilization of the lemma to the equation (\ref{eqn:rffgp_exp_naive}).

By replacing $\bs{A} = \sigma^2 \bs{I}$, $\bs{B} = \bs{\Phi}\tran$, $\bs{C} = \bs{\Phi}$,
and $\bs{D} = \bs{I}$ on the equation (\ref{eqn:matrix_inversion_lemma}),
we obtain the following equation:
\begin{equation}
    \left( \bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I} \right)^{-1}
    = \frac{1}{\sigma^2} \left (\bs{I} - \bs{\Phi}\tran
    \left( \bs{\Phi \Phi}\tran + \sigma^2 \bs{I} \right)^{-1} \bs{\Phi} \right),
    \label{eqn:app2_rffgp_exp_solving}
\end{equation}
where $\bs{P} = \bs{\Phi \Phi}\tran \in \mathbb{R}^{D \times D}$.
Then multiply $\bs{\Phi}$ from the right to the above equation (\ref{eqn:app2_rffgp_exp_solving}),
and we get
\begin{equation}
    \left( \bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I} \right)^{-1} \bs{\Phi}\tran
    = \frac{1}{\sigma^2} \bs{\Phi}\tran
    \left( \bs{I} - \left( \bs{P} + \sigma^2 \bs{I} \right)^{-1} \bs{P} \right).
    \label{eqn:app2_rff_key_eqn}
\end{equation}
Therefore, the expression (\ref{eqn:rffgp_exp_naive}) can be written as
\begin{equation}
    m (\bs{\xi}) = \widehat{m} (\bs{\xi}) + \frac{1}{\sigma^2}
    \left( \bs{y} - \widehat{\bs{m}} \right)\tran \bs{\Phi}\tran \bs{S} \bs{\phi}(\bs{\xi}),
    \label{eqn:app2_rffgp_exp}
\end{equation}
where
\begin{equation}
    \bs{S} = \bs{I} - \left( \bs{P} + \sigma^2 \bs{I} \right)^{-1} \bs{P}.
    \label{eqn:app2_rffgp_exp_cache}
\end{equation}

The same as the previous approach, the bottleneck has been resolved now.
The inverse matrix $\bigl( \bs{\Phi}\tran \bs{\Phi} + \sigma^2 \bs{I} \bigr)^{-1}$, which was
the bottleneck of the expression (\ref{eqn:rffgp_exp_naive}), became $(\bs {P} + \sigma^2 \bs{I})^{-1}$
in the expressions (\ref{eqn:app2_rffgp_exp}) and (\ref{eqn:app2_rffgp_exp_cache}) where the size of
the inverse matrix is $D \times D$. Normally, the RFF dimension $D$ is set sufficiently
smaller than the number of training data $N$, therefore the inverse matrix
$(\bs {P} + \sigma^2 \bs{I})^{-1}$ is no longer a bottleneck of computational cost.
The bottleneck of the expressions (\ref{eqn:app2_rffgp_exp}) and (\ref{eqn:app2_rffgp_exp_cache})
is the matrix product $\bs{P} = \bs{\Phi \Phi}\tran$, whose computational cost is $O(ND^2)$.
Therefore we've achieved a considerable speedup of the training of Gaussian process model
by applying RFF because the calculational cost before RFF is $O(N^3)$.

\subsection{Applying RFF to covariance of prediction}

Next, we apply RFF to the covariance of the prediction (\ref{eqn:gp_cov}).
By substituting RFF approximation (\ref{eqn:app2_rff_key_eqn}) to the expression (\ref{eqn:gp_cov}),
we obtain
\begin{align}
    v (\bs{\xi}_1, \bs{\xi}_2)
    & = \bs{\phi} (\bs{\xi}_1)\tran \bs{\phi} (\bs{\xi}_2)
    - \frac{1}{\sigma^2} \bs{\phi} (\bs{\xi}_1)\tran \bs{PS} \bs{\phi} (\bs{\xi}_2) \notag \\
    & = \bs{\phi} (\bs{\xi}_1)\tran
    \left( \bs{I} - \frac{1}{\sigma^2} \bs{PS} \right)
    \bs{\phi} (\bs{\xi}_2),
    \label{eqn:app2_rffgp_cov}
\end{align}
The bottleneck of the expression (\ref{eqn:rffgp_cov}) is, as well as the expectation of the
prediction, the matrix product $\bs{P} = \bs{\Phi \Phi}\tran$ whose calculation cost is $O(ND^2)$.

The procedure of training and inference of Gaussian process model after applying RFF is
described in the algorithms \ref{alg:app2_rffgp_pseudo_code} as pseudo-code.
Note that the prior distribution of Gaussian process model is set to 0 for the sake of
simplicity in the algorithms \ref{alg:app2_rffgp_pseudo_code}.

\begin{algorithm}[h]
    \caption{Training of the GP model after RFF (2)}
    \label{alg:app2_rffgp_pseudo_code}
    \begin{algorithmic}[1]
    \Function {training\_gp\_with\_rff}{$\mathcal{D}$, $\sigma$}
        \State $\bs{y} \gets (y_1, \ldots, y_N)\tran$
        \State $\bs{\Phi} \gets (\bs{\phi}(\bs{x}_1), \ldots, \bs{\phi}(\bs{x}_N))$
        \State $\bs{P} \gets \bs{\Phi\Phi}\tran$
        \State $\bs{S} \gets \bs{I} - \left( \bs{P} + \sigma^2 \bs{I} \right)^{-1} \bs{P}$
        \State $\bs{a} \gets \frac{1}{\sigma^2} \bs{y}\tran \bs{\Phi}\tran \bs{S}$
        \State $\bs{C} \gets \bs{I} - \frac{1}{\sigma^2} \bs{PS}$
        \State \Return ($\bs{a}$, $\bs{C}$)
    \EndFunction
    \State
    \Function {inference\_gp\_with\_rff}{$\mathcal{D}$, $\sigma$}
        \State $\bs{z} \gets \bs{\phi}(\bs{\xi})$
        \State $\mu \gets \bs{a}\tran \bs{z}$
        \State $v \gets \bs{z}\tran \bs{C} \bs{z}$
        \State \Return ($\mu$, $v$)
    \EndFunction
    \end{algorithmic}
\end{algorithm}

% }}}

\section{Appendix 2: Proofs}\titlebar
% {{{

\subsection{Proof of the lemma 1}

The lemma 1 is reprinted and proved.

\vspace*{6pt}
\begin{thmbox}[M]{\bfseries{Lemma 1} (inversion of the matrix $\bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}$)}
    Let $\bs{\Phi} \in \mathbb{R}^{D \times N}$ be a real matrix and $\eta$ be a positive real number.
    Then the matrices $\bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}_N$ and $\bs{\Phi} \bs{\Phi}\tran + \eta \bs{I}_D$
    are regular matrices and the equation
    \begin{equation}
        \left( \bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}_N \right)^{-1} \bs{\Phi}\tran
        = \bs{\Phi}\tran \left( \bs{\Phi} \bs{\Phi}\tran + \eta \bs{I}_D \right)^{-1}
    \end{equation}
    holds, where the matrices $\bs{I}_D$ and $\bs{I}_N$ are $D$ dimensional and $N$ dimensional
    identity matrices, respectively.
\end{thmbox}
\vspace*{4pt}
\begin{proof}
First, the matrices $\bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}_N$ and $\bs{\Phi} \bs{\Phi}\tran + \eta \bs{I}_D$
are positive definite, because
\begin{align*}
    \bs{x}\tran \left( \bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}_N \right) \bs{x} 
    &= \| \bs{\Phi x} \|^2 + \eta \| \bs{x} \|^2 > 0, \\
    \widehat{\bs{x}}\tran \left( \bs{\Phi} \bs{\Phi}\tran + \eta \bs{I}_D \right) \widehat{\bs{x}}
    &= \bigl\| \bs{\Phi}\tran \widehat{\bs{x}} \bigr\|^2 + \eta \| \widehat{\bs{x}} \|^2 > 0,
\end{align*}
holds for any non-zero real vectors $\bs{x} \in \mathbb{R}^N$ and $\widehat{\bs{x}} \in \mathbb{R}^D$.
In general, the determinant of a matrix is equal to the product of its eigenvalues.
Also, the eigenvalues of a positive definite matrix are always greater than zero.
Therefore, the determinant of a positive definite matrix is greater than zero.
Hence the matrices $\bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}_N$ and $\bs{\Phi} \bs{\Phi}\tran + \eta \bs{I}_D$
are regular matrices, that is, these matrices are invertible.

Next, it is obvious that the equation
\begin{equation*}
    \bs{\Phi}\tran \left( \bs{\Phi} \bs{\Phi}\tran + \eta \bs{I}_D \right)
    = \left( \bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}_N \right) \bs{\Phi}\tran,
\end{equation*}
holds, because both are equivalent to $\bs{\Phi}\tran \bs{\Phi} \bs{\Phi}\tran + \eta \bs{\Phi}\tran$.
You'll get the following equation by multiplying the inverse matrix
$\left( \bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}_N \right)^{-1}$ to the above equation from the left:
\begin{equation*}
    \left( \bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}_N \right)^{-1}
    \bs{\Phi}\tran \left( \bs{\Phi} \bs{\Phi}\tran + \eta \bs{I}_D \right)
    = \bs{\Phi}\tran.
\end{equation*}
Similarly, by multiplying the inverse matrix $\left( \bs{\Phi} \bs{\Phi}\tran + \eta \bs{I}_D \right)^{-1}$
to the above equation from the right, you'll get
\begin{equation*}
    \left( \bs{\Phi}\tran \bs{\Phi} + \eta \bs{I}_N \right)^{-1} \bs{\Phi}\tran
    = \bs{\Phi}\tran \left( \bs{\Phi} \bs{\Phi}\tran + \eta \bs{I}_D \right)^{-1}.
\end{equation*}
\end{proof}

\subsection{Proof of the matrix inversion lemma}

The matrix inversion lemma is reprinted and proved.

\vspace*{6pt}
\begin{thmbox}[M]{\bfseries{Lemma 2 (Matrix Inversion Lemma)}}
    Let
    $\bs{A} \in \mathbb{R}^{N \times N}$,
    $\bs{B} \in \mathbb{R}^{N \times M}$,
    $\bs{C} \in \mathbb{R}^{M \times N}$,
    and
    $\bs{D} \in \mathbb{R}^{M \times M}$
    be real matrices. Then the equation
    \begin{equation}
        \left( \bs{A} + \bs{BDC} \right)^{-1} = \bs {A}^{-1} - \bs{A}^{-1} \bs{B}
        \left( \bs{D}^{-1} + \bs{CA}^{-1} \bs{B} \right)^{-1} \bs{CA}^{-1}
    \end{equation}
    holds, where the matrices $\bs{A}$ and $\bs{D}$ are regular matrices.
\end{thmbox}
\vspace*{4pt}
\begin{proof}
The following equation holds:
\begin{align*}
    \begin{pmatrix}
        \bs{A} & \bs{B} \\
        \bs{C} & \bs{D}
    \end{pmatrix}^{-1}
    &= \begin{pmatrix}
        \bs{A}^{-1} + \bs{A}^{-1} \bs{BSCA}^{-1} & - \bs{A}^{-1} \bs{BS} \\
        - \bs {SCA}^{-1}                         & \bs {S}
    \end{pmatrix} \\
    &= \begin{pmatrix}
        \bs{T}                & -\bs{TBD}^{-1} \\
        - \bs{D}^{-1} \bs{CT} & \bs{D}^{-1} + \bs{D}^{-1} \bs{CTBD}^{-1}
    \end{pmatrix},
\end{align*}
where
\begin{align}
    \bs{T} &= \left( \bs{D} - \bs {CA}^{-1} \bs{B} \right)^{-1}, \\
    \bs{S} &= \left( \bs{A} - \bs {BD}^{-1} \bs{C} \right)^{-1}.
\end{align}
It is easy to verify the above equation from a direct calculation.
By comparing the corresponding parts of the above block matrix, we get
\begin{align}
    \bs{T} &= \bs{A}^{-1} + \bs{A}^{-1} \bs{BSCA}^{-1},
    \label{eqn:binomial_theorem_proof_1} \\
    \bs{S} &= \bs{D}^{-1} + \bs{D}^{-1} \bs{CTBD}^{-1}, \\
    - \bs{A}^{-1} \bs{BS} &= - \bs{TBD}^{-1}, \\
    - \bs{SCA}^{-1} &= - \bs{D}^{-1} \bs{CT},
\end{align}
By replacing with
\begin{center}
    $\bs{A} \to \bs{D}^{-1}$, \hspace{5pt}
    $\bs{B} \to -\bs{C}$, \hspace{5pt}
    $\bs{C} \to \bs{B}$, \hspace{5pt}
    $\bs{D} \to \bs{A}$,
\end{center}
in the equation (\ref{eqn:binomial_theorem_proof_1}), we get the formula to be proved.
\end{proof}

% }}}

\newpage

\begin{thebibliography}{9}
% {{{

    \bibitem{Rahimi2007}
    A.~Rahimi and B.~Recht, 
    ``Random Features for Large-Scale Kernel Machines'',
    Neural Information Processing Systems, 2007.

    \bibitem{Rasmussen2006}
    C.~Rasmussen and C.~Williams, ``Gaussian Processes for Machine Learning'', MIT Press, 2006.

    \bibitem{Ishikawa2021}
    \texttt{https://github.com/tiskw/mathematical-articles}

% }}}
\end{thebibliography}

\end{document}

% vim: expandtab tabstop=4 shiftwidth=4 fdm=marker
