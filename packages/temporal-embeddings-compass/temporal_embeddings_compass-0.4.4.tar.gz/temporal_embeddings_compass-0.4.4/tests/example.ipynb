{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d4n1elp/.local/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/d4n1elp/.local/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/d4n1elp/.local/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/d4n1elp/.local/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from words import interesting_, interesting_word_list # Word list if necessary\n",
    "from cade import TTEC_wrapper, load, save, TTEC, TimeSlice\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train TTEC\n",
    "## Train using sequential approach\n",
    "For this method, you need to have a list of timestamps and a text file of preprocessed documents.\n",
    "It will then separate the documents by year and automatically create time slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1323.1047928333282 seconds\n"
     ]
    }
   ],
   "source": [
    "# Basic logging\n",
    "logging.basicConfig(filename='log_sequential.txt', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# List of timestamps, loaded from pickle format\n",
    "stamps = load('data/years.pkl')\n",
    "start = time.time()\n",
    "wrapper = TTEC_wrapper(corpus_file='data/all_preprocessed.txt', time_stamps=stamps, size=100,n_topics=50,\n",
    "                      train_compass_now=True, train_slices_now=True, min_count=50,\n",
    "                      hdbscan_selection=\"nested\", similarity_method=\"vote\"\n",
    "                      )\n",
    "print(\"Time taken:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: coh -0.06024703269064913 div 0.9361531122242683\n",
      "40: coh -0.06480162497217568 div 0.9465820926364688\n",
      "30: coh -0.06651853685715607 div 0.9616254041698884\n",
      "20: coh -0.08775501453140756 div 0.9780729566814474\n",
      "10: coh -0.1531354895126527 div 0.9938194810769568\n"
     ]
    }
   ],
   "source": [
    "for n_topics in [50, 40, 30, 20, 10]:\n",
    "    wrapper.reduce_topics(n_topics)\n",
    "    print(f\"{n_topics}: coh {wrapper.test_coherence()} div {wrapper.test_diversity()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using parallel approach\n",
    "For this method, you need to have a text file with all the preprocessed documents and then a series of text files which represent each time slice.\n",
    "TTEC will train a compass first and then train the other time slices in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 621.3934881687164 seconds\n"
     ]
    }
   ],
   "source": [
    "def executor_function(year):\n",
    "    return ttec.train_slice(f'data/{year}.txt', create_topics=False), year\n",
    "\n",
    "stamps = load('data/years.pkl')\n",
    "start = time.time()\n",
    "ttec = TTEC(hdbscan_selection=\"nested\", n_topics=50, log=True, log_name='log_parallel.log')\n",
    "ttec.train_compass('data/all_preprocessed.txt')\n",
    "wrapper = TTEC_wrapper(corpus_file='data/all_preprocessed.txt', time_stamps=stamps, size=100,n_topics=50,\n",
    "                      train_compass_now=False, train_slices_now=False, min_count=50,\n",
    "                      hdbscan_selection=\"nested\", similarity_method=\"vote\"\n",
    "                      )\n",
    "wrapper.compass = ttec\n",
    "with ProcessPoolExecutor(max_workers=6) as executor:\n",
    "    for result in executor.map(executor_function, range(7,16)):\n",
    "        wrapper.slices[result[1]] = result[0]\n",
    "wrapper.remake_slice_embeddings()\n",
    "print(\"Time taken:\", time.time() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: coh -0.07374714343547341 div 0.9324240117722551\n",
      "40: coh -0.06848750635673428 div 0.94896520145362\n",
      "30: coh -0.06771502821288779 div 0.9591100529133316\n",
      "20: coh -0.09309892628740647 div 0.9793894074129125\n",
      "10: coh -0.1278014950072993 div 0.9941041924164288\n"
     ]
    }
   ],
   "source": [
    "for n_topics in [50, 40, 30, 20, 10]:\n",
    "    wrapper.reduce_topics(n_topics)\n",
    "    print(f\"{n_topics}: coh {wrapper.test_coherence()} div {wrapper.test_diversity()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
