# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import TYPE_CHECKING, List, Union, Optional
from typing_extensions import Literal, TypeAlias

from ...._models import BaseModel

__all__ = [
    "ChatCompletionResponse",
    "Choice",
    "ChoiceMessage",
    "ChoiceMessageToolCall",
    "ChoiceMessageToolCallFunction",
    "ChoiceLogprobs",
    "ChoiceLogprobsContent",
    "ChoiceLogprobsContentTopLogprob",
    "Usage",
    "UsageCompletionTokensDetails",
    "UsageCompletionTokensDetailsCompletionTokensDetailsWrapper",
    "UsagePromptTokensDetails",
    "UsagePromptTokensDetailsPromptTokensDetailsWrapper",
]


class ChoiceMessageToolCallFunction(BaseModel):
    name: str
    """The name of the function."""

    arguments: Optional[str] = None
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    type: Optional[str] = None
    """The type of the function."""


class ChoiceMessageToolCall(BaseModel):
    id: str
    """The ID of the tool call."""

    function: Optional[ChoiceMessageToolCallFunction] = None
    """The function that was called."""

    type: Optional[Literal["function"]] = None
    """The type of the tool call."""


class ChoiceMessage(BaseModel):
    content: Optional[str] = None
    """The content of the message."""

    refusal: Optional[str] = None
    """The refusal message generated by the model."""

    role: Optional[str] = None
    """
    The role of the author of this message.This field is only relevant if the
    completion is part of a multi-turn conversation.
    """

    tool_calls: Optional[List[ChoiceMessageToolCall]] = None
    """Tool calls generated by the model."""


class ChoiceLogprobsContentTopLogprob(BaseModel):
    token: str

    logprob: float

    bytes: Optional[List[int]] = None


class ChoiceLogprobsContent(BaseModel):
    token: str

    logprob: float

    top_logprobs: List[ChoiceLogprobsContentTopLogprob]

    bytes: Optional[List[int]] = None


class ChoiceLogprobs(BaseModel):
    content: Optional[List[ChoiceLogprobsContent]] = None
    """A list of message content tokens with log probability information."""


class Choice(BaseModel):
    index: str
    """The index of the choice."""

    message: ChoiceMessage
    """The message generated by the model."""

    finish_reason: Optional[str] = None
    """The reason the completion stopped."""

    logprobs: Optional[ChoiceLogprobs] = None
    """Log probabilities of the tokens generated."""


class UsageCompletionTokensDetailsCompletionTokensDetailsWrapper(BaseModel):
    audio_tokens: Optional[int] = None

    reasoning_tokens: Optional[int] = None

    text_tokens: Optional[int] = None

    if TYPE_CHECKING:
        # Stub to indicate that arbitrary properties are accepted.
        # To access properties that are not valid identifiers you can use `getattr`, e.g.
        # `getattr(obj, '$type')`
        def __getattr__(self, attr: str) -> object: ...


UsageCompletionTokensDetails: TypeAlias = Union[UsageCompletionTokensDetailsCompletionTokensDetailsWrapper, object]


class UsagePromptTokensDetailsPromptTokensDetailsWrapper(BaseModel):
    audio_tokens: Optional[int] = None

    cached_tokens: Optional[int] = None

    image_tokens: Optional[int] = None

    text_tokens: Optional[int] = None

    if TYPE_CHECKING:
        # Stub to indicate that arbitrary properties are accepted.
        # To access properties that are not valid identifiers you can use `getattr`, e.g.
        # `getattr(obj, '$type')`
        def __getattr__(self, attr: str) -> object: ...


UsagePromptTokensDetails: TypeAlias = Union[UsagePromptTokensDetailsPromptTokensDetailsWrapper, object]


class Usage(BaseModel):
    completion_tokens: Optional[int] = None
    """The number of tokens in the output completion."""

    completion_tokens_details: Optional[UsageCompletionTokensDetails] = None
    """Breakdown of tokens used in the prompt."""

    prompt_tokens: Optional[int] = None
    """The number of tokens in the generated completion."""

    prompt_tokens_details: Optional[UsagePromptTokensDetails] = None
    """Breakdown of tokens used in a completion."""

    total_tokens: Optional[int] = None
    """The total number of tokens in the prompt and completion."""


class ChatCompletionResponse(BaseModel):
    choices: List[Choice]

    id: Optional[str] = None
    """A unique identifier for the completion."""

    created: Optional[int] = None
    """The Unix timestamp (in seconds) of when the chat completion was created."""

    model: Optional[str] = None
    """The model used for completion."""

    object: Optional[str] = None
    """The object type, ex `text_completion` or `chat.completion`"""

    system_fingerprint: Optional[str] = None
    """This fingerprint represents the backend configuration that the model runs with."""

    usage: Optional[Usage] = None
    """Usage statistics."""
