"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from emissary_client_sdk.types import BaseModel
from emissary_client_sdk.utils import FieldMetadata, PathParamMetadata, RequestMetadata
from enum import Enum
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class Role(str, Enum):
    r"""The role of the message sender"""

    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"


class MessagesTypedDict(TypedDict):
    role: NotRequired[Role]
    r"""The role of the message sender"""
    content: NotRequired[str]
    r"""The content of the message"""


class Messages(BaseModel):
    role: Optional[Role] = None
    r"""The role of the message sender"""

    content: Optional[str] = None
    r"""The content of the message"""


class GetChatCompletionsFromDeploymentRequestBodyTypedDict(TypedDict):
    r"""Provide your chat input for completions"""

    messages: List[MessagesTypedDict]
    streaming: NotRequired[bool]
    r"""Whether to stream the response"""
    temperature: NotRequired[float]
    r"""The sampling temperature to use"""
    max_new_tokens: NotRequired[int]
    r"""The maximum number of new tokens to generate"""
    top_p: NotRequired[float]
    r"""The cumulative probability for token selection"""
    top_k: NotRequired[int]
    r"""The number of highest probability tokens to keep for top-k-filtering"""
    no_repeat_ngram_size: NotRequired[int]
    r"""The size of n-grams to avoid repeating"""
    do_sample: NotRequired[bool]
    r"""Whether to use sampling or not"""


class GetChatCompletionsFromDeploymentRequestBody(BaseModel):
    r"""Provide your chat input for completions"""

    messages: List[Messages]

    streaming: Optional[bool] = None
    r"""Whether to stream the response"""

    temperature: Optional[float] = None
    r"""The sampling temperature to use"""

    max_new_tokens: Optional[int] = None
    r"""The maximum number of new tokens to generate"""

    top_p: Optional[float] = None
    r"""The cumulative probability for token selection"""

    top_k: Optional[int] = None
    r"""The number of highest probability tokens to keep for top-k-filtering"""

    no_repeat_ngram_size: Optional[int] = None
    r"""The size of n-grams to avoid repeating"""

    do_sample: Optional[bool] = None
    r"""Whether to use sampling or not"""


class GetChatCompletionsFromDeploymentRequestTypedDict(TypedDict):
    project_id: str
    r"""The ID of the project to retrieve deployments for"""
    deployment_id: str
    r"""The ID of the deployment to get chat completions from"""
    request_body: GetChatCompletionsFromDeploymentRequestBodyTypedDict
    r"""Provide your chat input for completions"""


class GetChatCompletionsFromDeploymentRequest(BaseModel):
    project_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The ID of the project to retrieve deployments for"""

    deployment_id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""The ID of the deployment to get chat completions from"""

    request_body: Annotated[
        GetChatCompletionsFromDeploymentRequestBody,
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ]
    r"""Provide your chat input for completions"""


class GetChatCompletionsFromDeploymentResponseBodyTypedDict(TypedDict):
    r"""Successful operation"""

    id: NotRequired[str]
    r"""The unique identifier for the chat completion"""
    response: NotRequired[str]
    r"""The generated chat completion from the deployment"""


class GetChatCompletionsFromDeploymentResponseBody(BaseModel):
    r"""Successful operation"""

    id: Optional[str] = None
    r"""The unique identifier for the chat completion"""

    response: Optional[str] = None
    r"""The generated chat completion from the deployment"""
