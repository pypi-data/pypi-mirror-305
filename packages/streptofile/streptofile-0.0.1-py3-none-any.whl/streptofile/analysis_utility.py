# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_analysis_utility.ipynb.

# %% auto 0
__all__ = ['analysis_utility', 'get_names_from_fasta', 'analysis_manager', 'print_analysis_options', 'update_cleanup_config',
           'update_cli_input_config', 'unit_test_single', 'unit_test_single_2', 'unit_test_single_3',
           'unit_test_from_folder', 'unit_test_from_samplesheet']

# %% ../nbs/02_analysis_utility.ipynb 2
# That export there, it makes sure this code goes into the module.

# standard libs
import os
import re

# Common to template
# add into settings.ini, requirements, package name is python-dotenv, for conda build ensure `conda config --add channels conda-forge`
import dotenv  # for loading config from .env files, https://pypi.org/project/python-dotenv/
import envyaml  # Allows to loads env vars into a yaml file, https://github.com/thesimj/envyaml
import fastcore  # To add functionality related to nbdev development, https://github.com/fastai/fastcore/
from fastcore import (
    test,
)
from fastcore.script import (
    call_parse,
)  # for @call_parse, https://fastcore.fast.ai/script
import json  # for nicely printing json and yaml
from fastcore import test
from fastcore.script import call_parse
from streptofile import (
    core,
    sample_manager,
    convert_external_genome,
)
from pathlib import Path  # to be able write :Path in cli function

# Project specific libraries
import subprocess
import pandas as pd
import logging
import csv

# %% ../nbs/02_analysis_utility.ipynb 5
class analysis_utility(sample_manager.sample_data):

    log_file_name = "analysis.log"

    def __init__(self, attributes, input_folder, output_folder, analysis_config):
        attributes = attributes.copy()
        super().__init__(attributes, input_folder)
        self.analysis_results = {}
        self.analysis_output_files = {}
        self.output_folder = output_folder
        self.analysis_config = analysis_config
        self.check_path_existence()

    ### Structural functions
    ## This section includes utility functions for setting up folders, setting up log files, writing results to tsv etc.

    # Setup up log filehandle for sample
    def setup_sample_logging(self):
        self.logger = logging.getLogger()
        log_file = os.path.join(self.output_folder, self.log_file_name)
        self.log_filehandle = logging.FileHandler(log_file, mode="w", encoding="utf-8")
        self.log_filehandle.setFormatter(
            logging.Formatter(
                fmt="{asctime} - {levelname} - {message}",
                style="{",
                datefmt="%Y-%m-%d %H:%M:%S",
            )
        )
        self.logger.addHandler(self.log_filehandle)
        self.logger.setLevel(logging.INFO)

    # Check if input paths (assembly file, Illumina read files, Nanopore read files) exist on the system. Set to None if not.
    def check_path_existence(self):
        if not self.assembly_file is None and not os.path.exists(self.assembly_file):
            self.logger.warning(
                f"Provided assembly file does not exist at {self.assembly_file}. Analyses dependent on assembly input will be skipped"
            )
            self.assembly_file = None
        if not self.Illumina_read_files is None and not os.path.exists(
            self.Illumina_read_files[0]
        ):
            self.logger.warning(
                f"Provided paired end R1 file does not exist at {self.Illumina_read_files[0]}. Analyses dependent on paired end input will be skipped"
            )
            self.Illumina_read_files = None
        if not self.Illumina_read_files is None and not os.path.exists(
            self.Illumina_read_files[1]
        ):
            self.logger.warning(
                f"Provided paired end R2 file does not exist at {self.Illumina_read_files[1]}. Analyses dependent on paired end input will be skipped"
            )
            self.Illumina_read_files = None
        if not self.Nanopore_read_file is None and not os.path.exists(
            self.Nanopore_read_file
        ):
            self.logger.warning(
                f"Provided Nanopore data file does not exist at {self.Nanopore_read_file}. Analyses dependent on Nanopore input will be skipped"
            )
            self.Nanopore_read_file = None

    # Set up folder to store results for specific analysis. Default: [output_folder]/[sample_name]/[analysis_alias]
    def analysis_setup(self, analysis_alias, output_folder=False):
        self.logger.info(
            f"------------------------------------------------------------------------------------------"
        )
        self.logger.info(f"RUNNING ANALYSIS: {analysis_alias}")
        if not output_folder:
            output_folder = os.path.join(self.output_folder, analysis_alias)
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)
        if analysis_alias in self.analysis_results:
            self.logger.warning(
                f"An analysis with name {analysis_alias} has already been performed. Outputs in {output_folder} may be overwritten."
            )
        self.logger.info(
            f"Printing {analysis_alias} results for {self.sample_name} to {output_folder}"
        )
        self.analysis_results[analysis_alias] = {}
        self.analysis_output_files[analysis_alias] = {}
        return output_folder

    # Set up folder for sample
    def sample_setup(self):
        if not os.path.exists(self.output_folder):
            try:
                os.makedirs(self.output_folder)
            except Exception as e:
                print(
                    f"Failed to create folder for {self.sample_name} in {self.output_folder}. {e}"
                )
                # self.logger.critical(f"Failed to create folder for {self.sample_name} in {self.output_folder}. {e}")
        self.setup_sample_logging()
        self.logger.info(
            f"Running analyses on {self.sample_name} in {self.output_folder}"
        )
        self.logger.info(f"Inputs:")
        if not self.assembly_file is None:
            self.logger.info(f"Assembly file: {self.assembly_file}")
        if not self.Illumina_read_files is None:
            self.logger.info(
                f"Illumina read files: {', '.join(self.Illumina_read_files)}"
            )
        if not self.Nanopore_read_file is None:
            self.logger.info(f"Nanopore read file: {self.Nanopore_read_file}")
        self.logger.info(
            f"------------------------------------------------------------------------------------------"
        )
        self.logger.info(
            f"Analyses to run: {', '.join(self.analysis_config['analyses_to_run'])}"
        )

    # Close log filehandle for sample logging
    def sample_cleanup(self):
        self.logger.removeHandler(self.log_filehandle)
        self.log_filehandle.close()

    # Clean intermediate outputs for an analysis
    # This should be run as the last step of any analysis after updating self.analysis_results
    def analysis_cleanup(self, analysis_alias, files_to_clean):
        self.logger.info(f"Cleaning temporary files from analysis {analysis_alias}")
        output_files = self.analysis_output_files[analysis_alias]
        output_file_folders = set()
        for file_to_clean in files_to_clean:
            if os.path.exists(output_files[file_to_clean]):
                self.logger.info(f"Removing {output_files[file_to_clean]}")
                os.remove(output_files[file_to_clean])
            output_file_folders.add(os.path.dirname(output_files[file_to_clean]))
        for path in output_file_folders:
            if (
                os.path.exists(path)
                and not os.path.isfile(path)
                and not os.listdir(path)
            ):
                try:
                    self.logger.info(
                        f"Cleaned all files from {path}. Removing empty directory"
                    )
                    os.rmdir(path)
                except Exception as e:
                    self.logger.warning(
                        f"Attempt to remove directory {path} failed. {e}"
                    )

    # Print results from all analyses to tsv-file. Includes metadata by default.
    def write_to_tsv(self, include_metadata=True):
        output_file = os.path.join(self.output_folder, "results_summary.tsv")
        if include_metadata:
            print_dict = self.metadata.copy()
            if "Illumina_read_files" in print_dict and isinstance(
                print_dict["Illumina_read_files"], list
            ):
                print_dict["Illumina_read_files"] = ",".join(
                    print_dict["Illumina_read_files"]
                )
            if "sample_name" in print_dict:
                print_dict = {
                    "sample_name": print_dict.pop("sample_name"),
                    **print_dict,
                }  # Move sample_name to first column before printing for readability
        else:
            if "sample_name" in print_dict:
                print_dict = {"sample_name": self.sample_name}
            else:
                print_dict = {}
        for analysis_alias in self.analysis_results:
            print_dict.update(self.analysis_results[analysis_alias])
        try:
            with open(output_file, "w") as o:
                dict_writer = csv.DictWriter(
                    o, fieldnames=list(print_dict.keys()), delimiter="\t"
                )
                dict_writer.writeheader()
                dict_writer.writerows([print_dict])
            o.close()
            self.logger.info(f"Analysis summary written to {output_file}")
        except Exception as e:
            self.logger.critical(
                f"ERROR while writing analysis summary to {output_file}: {e}"
            )

    # Execute a shell command and log stderr and stdout automatically
    def execute_cmd_and_log(self, cmd, log_stdout=True):
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            shell=True,
            encoding="utf-8",
        )
        stdout, stderr = process.communicate()
        self.logger.info(f"Running command: {cmd}")
        if log_stdout and stdout and stdout is not None:
            self.logger.info(f"Shell command STDOUT: {stdout}")
        if stderr and stderr is not None:
            self.logger.error(f"Shell command STDERR: {stderr}")
        return (stdout, stderr)

    ### Analysis functions

    ## Below are functions involved in running individual analyses

    # Map paired end Illumina reads to provided reference with bwa mem and do variant calling with gatk. Returns path to vcf-file.
    # VCF output can be used for lineage determination when mapping to complete genome and for checking for SNPs in specific genes (._get_SNPs_from_vcf_)
    # VCF output can be used directly in nasp
    def _Illumina_read_mapping(
        self, analysis_alias, reference_fasta_file, output_folder=False
    ):
        if not output_folder:
            output_folder = os.path.join(self.output_folder, analysis_alias)
        bam_prefix = os.path.join(output_folder, self.sample_name + "-bwamem")
        bam_file = bam_prefix + ".bam"
        vcf_file = os.path.join(output_folder, self.sample_name + ".vcf")
        bwamem_cmd = f"bwa mem -R '@RG\\tID:{self.sample_name}\\tSM:{self.sample_name}'  -t 4 {reference_fasta_file} {self.Illumina_read_files[0]} {self.Illumina_read_files[1]} | samtools view -S -b -h - | samtools sort -o {bam_file}"
        samtoolsindex_cmd = f"samtools index {bam_file}"
        gatk_cmd = f"java -Xmx10G -jar GenomeAnalysisTK.jar -T UnifiedGenotyper -dt NONE -glm BOTH -I {bam_file} -R {reference_fasta_file} -nt 4 -o {vcf_file} -out_mode EMIT_ALL_CONFIDENT_SITES -baq RECALCULATE -stand_call_conf 100 -ploidy 1"
        commands = {}
        if os.path.exists(vcf_file):
            self.logger.info(f"vcf file found at {vcf_file}. Skipping read mapping")
        else:
            if os.path.exists(bam_file + ".bai"):
                cmd = gatk_cmd
                self.logger.info(
                    f"Indexed bam file found at {bam_file}.bai. Running gatk."
                )
            else:
                if os.path.exists(bam_file):
                    cmd = f"{samtoolsindex_cmd}; {gatk_cmd}"
                    self.logger.info(
                        f"Bam file found at {bam_file}. Indexing and running gatk."
                    )
                else:
                    cmd = f"{bwamem_cmd}; {samtoolsindex_cmd}; {gatk_cmd}"
                    self.logger.info(f"Running bwamem and gatk")

            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                shell=True,
                encoding="utf-8",
            )
            commands[cmd] = process.communicate()
            self.analysis_output_files[analysis_alias] = {
                "vcf": vcf_file,
                "bam": bam_file,
                "bam_index": bam_file + ".bai",
            }
        return vcf_file

    # DEPENDENCY: Output file from ._Illumina_read_mapping (.vcf)
    # Loads genotype variants found in tsv-file and parses a provided vcf-file (from ._Illumina_read_mapping) for variants matching those listed in the genotype_variant_table
    # Returns three dataframes: One containing all variants listed in the genotype_variant file, one containing the complete vcf-file, and one containing variants found in both
    def get_SNPs_from_vcf(self, analysis_alias, genotype_variant_table, vcf_file):
        vcf_df = pd.read_csv(vcf_file, sep="\t", header=None, comment="#")
        vcf_df.columns = [
            "referenceID",
            "position",
            "ID",
            "ref",
            "alt",
            "qual",
            "filter",
            "info",
            "format",
            "count",
        ]
        LOC_df = pd.read_csv(genotype_variant_table, sep="\t")
        LOC_df["variant"] = (
            LOC_df["referenceID"]
            + "::"
            + LOC_df["ref"]
            + LOC_df["position"].astype(str)
            + LOC_df["alt"]
        )
        vcf_df["variant"] = (
            vcf_df["referenceID"]
            + "::"
            + vcf_df["ref"]
            + vcf_df["position"].astype(str)
            + vcf_df["alt"]
        )
        variants_filter_vals = LOC_df["variant"]
        vcf_df_filtered = vcf_df.query("variant in @variants_filter_vals")
        variants_found = list(vcf_df_filtered["variant"])
        LOC_df_filtered = LOC_df.query("variant in @variants_found")

        genotype_SNP_counts = {}
        for genotype in set(LOC_df["genotype"]):
            try:
                print(
                    f"{genotype} {LOC_df_filtered['genotype'].value_counts()[genotype]} {LOC_df['genotype'].value_counts()[genotype]}"
                )
                genotype_SNP_counts[genotype] = {}
            except KeyError:
                print(f"{genotype} 0 {LOC_df['genotype'].value_counts()[genotype]}")
        return (LOC_df, vcf_df, LOC_df_filtered)

    # Map assembly sequences to provided reference. Returns "frankenfasta" file as known from NASP
    # Frankenfasta output can be used for lineage determination when mapping to complete genome and for checking for SNPs in specific genes (._get_SNPs_from_frankenfasta)
    # Frankenfasta output is already aligned to reference. To obtain a core genome alignment from multiple isolates just do "cat *.frankenfasta > core_genome_alignment.fasta"
    def _nucmer_mapping(
        self, analysis_alias, reference_fasta_file, output_folder=False
    ):
        if not output_folder:
            output_folder = os.path.join(self.output_folder, analysis_alias)
        external_genome = convert_external_genome.Genome()
        external_genome.import_fasta_file(self.assembly_file)
        prefix = os.path.join(output_folder, self.sample_name)
        delta_file = f"{prefix}.delta"
        filtered_delta_file = f"{prefix}.filtered.delta"
        frankenfasta_file = f"{prefix}.frankenfasta"
        if not os.path.exists(frankenfasta_file):
            self.logger.info(
                f"Running nucmer and writing frankenfasta file to {frankenfasta_file}"
            )
            delta_cmd = (
                f"nucmer --prefix={prefix} {reference_fasta_file} {self.assembly_file}"
            )
            delta_filter_cmd = (
                f"delta-filter -q -r -o 100 {delta_file} > {filtered_delta_file}"
            )

            stdout, stderr = self.execute_cmd_and_log(delta_cmd, log_stdout=False)
            stdout, stderr = self.execute_cmd_and_log(delta_filter_cmd)
            external_genome = convert_external_genome.Genome()
            external_genome.import_fasta_file(self.assembly_file)
            franken_genome = convert_external_genome.Genome()
            convert_external_genome.parse_delta_file(
                (filtered_delta_file), franken_genome, external_genome
            )
            franken_genome.write_to_fasta_file(
                (frankenfasta_file), self.sample_name + " ref:"
            )
        else:
            self.logger.info(
                f"Frankenfasta file already exists at {frankenfasta_file}, skipping nucmer alignment"
            )
        self.analysis_output_files[analysis_alias].update(
            {
                "frankenfasta": frankenfasta_file,
                "delta": delta_file,
                "filtered_delta": filtered_delta_file,
            }
        )
        return frankenfasta_file

    ### DEPENDENCY: output file from _nucmer_mapping (.frankenfasta)
    ### Parses frankenfasta file for specific SNPs supplied in a tsv-file (variant_table_file)
    def get_SNPs_from_fasta(self, variant_table_file, mapped_fasta_file):
        fasta_dict = {}
        with open(mapped_fasta_file) as f:
            for line in f:
                line = line.rstrip("\n")
                if line[0] == ">":
                    header = line.split("ref:")[1]
                    fasta_dict[header] = ""
                else:
                    fasta_dict[header] += line
        LOC_df = pd.read_csv(variant_table_file, sep="\t")
        LOC_df["variant"] = (
            LOC_df["referenceID"]
            + "::"
            + LOC_df["ref"]
            + LOC_df["position"].astype(str)
            + LOC_df["alt"]
        )

        LOC_df_dict = LOC_df.to_dict("index")
        variants_filter_vals = LOC_df["variant"]
        variants_found = []
        for idx in LOC_df_dict:
            if LOC_df_dict[idx]["referenceID"] in fasta_dict:
                if (
                    fasta_dict[LOC_df_dict[idx]["referenceID"]][
                        (LOC_df_dict[idx]["position"] - 1)
                    ]
                    == LOC_df_dict[idx]["alt"]
                ):
                    variants_found.append(LOC_df_dict[idx]["variant"])

        LOC_df_filtered = LOC_df.query("variant in @variants_found")

        return (LOC_df, LOC_df_filtered)

    def _assembly_lineage_determination_(
        self, lineage_determination_config, mapped_fasta_file=False, output_folder=False
    ):
        analysis_alias = lineage_determination_config["alias"]
        if self.assembly_file is None:
            self.logger.critical(
                f"Assembly file not provided or not found. Skipping analysis {analysis_alias}"
            )
            self.analysis_results[analysis_alias] = {analysis_alias: "NA"}
        else:
            output_folder = self.analysis_setup(analysis_alias, output_folder)
            reference_fasta_file = lineage_determination_config["reference_fasta_file"]
            variant_table_file = lineage_determination_config["lineage_variant_file"]
            if not mapped_fasta_file or not os.path.exists(mapped_fasta_file):
                mapped_fasta_file = self._nucmer_mapping(
                    analysis_alias, reference_fasta_file
                )
            all_variants, found_variants = self.get_SNPs_from_fasta(
                variant_table_file, mapped_fasta_file
            )
            lineage_SNP_counts = {}
            for genotype in set(all_variants["genotype"]):
                try:
                    lineage_SNP_counts[genotype] = (
                        found_variants["genotype"].value_counts()[genotype]
                        / all_variants["genotype"].value_counts()[genotype]
                        * 100
                    )
                except KeyError:
                    lineage_SNP_counts[genotype] = 0
            best_hit = max(lineage_SNP_counts, key=lineage_SNP_counts.get)
            if (
                not lineage_SNP_counts[best_hit]
                > lineage_determination_config["percent_snp_threshold"]
            ):
                best_hit = "-"
            self.analysis_results[analysis_alias].update({"Lineage": best_hit})
            self.analysis_cleanup(
                analysis_alias, lineage_determination_config["files_to_clean"]
            )

    # Blast sequence in query fasta file and parse the output. A gene is considered present if above the threshold specified in config (default: 90 % identity, 90% length)
    def _blast_presence_absence_(
        self, blast_presence_absence_config: dict, output_folder=False
    ):
        analysis_alias = blast_presence_absence_config["alias"]
        if self.assembly_file is None:
            self.logger.critical(
                f"Assembly file not provided or not found. Skipping analysis {analysis_alias}"
            )
            self.analysis_results[analysis_alias] = {analysis_alias: "NA"}
        else:
            output_folder = self.analysis_setup(analysis_alias, output_folder)
            query_sequence_file = blast_presence_absence_config["query_fasta_file"]
            blast_output_file = os.path.join(output_folder, "blast_output.tsv")
            cov_threshold = blast_presence_absence_config["cov_threshold"]
            pident_threshold = blast_presence_absence_config["pident_threshold"]
            gene_presence_absence = {}
            genes_found = []
            gene_names = get_names_from_fasta(query_sequence_file)
            if not os.path.exists(blast_output_file):
                cmd = f"blastn -query {query_sequence_file} -subject {self.assembly_file} -out {blast_output_file} -outfmt \"6 {blast_presence_absence_config['blast_header']}\" {blast_presence_absence_config['additional_blast_parameters']}"
                stdout, stderr = self.execute_cmd_and_log(cmd)
            else:
                self.logger.info(
                    f"Blast output found at {blast_output_file}, skipping blast."
                )
            self.logger.info(f"Parsing blast output in {blast_output_file}")
            try:
                blast_df = pd.read_csv(blast_output_file, sep="\t", header=None)
            except Exception as e:
                self.logger.info(
                    f"Failed to parse blast output in {blast_output_file}, error message: {e}"
                )
                blast_df = None
            if blast_df is not None:
                blast_df.columns = blast_presence_absence_config["blast_header"].split(
                    " "
                )
                blast_df["plen"] = blast_df["length"] / blast_df["qlen"] * 100
                blast_df_filtered = blast_df.query(
                    "plen > @cov_threshold and pident > @pident_threshold"
                )
                blast_df_unique = (
                    blast_df_filtered.sort_values(by=["bitscore"], ascending=False)
                    .groupby("qseqid")
                    .first()
                )
                genes_found = ",".join(sorted(list(blast_df_unique.index)))
                genes_present = {f"{analysis_alias}_genes_found": genes_found}

            else:
                blast_df_unique = None
            for gene_name in gene_names:
                if gene_name in genes_found:
                    gene_presence_absence[gene_name] = "1"
                else:
                    gene_presence_absence[gene_name] = "0"
            if blast_presence_absence_config["results_format"] == "string":
                self.analysis_results[analysis_alias] = genes_present
            else:
                self.analysis_results[analysis_alias] = gene_presence_absence
            self.analysis_output_files[analysis_alias] = {"blast": blast_output_file}
            self.analysis_cleanup(
                analysis_alias, blast_presence_absence_config["files_to_clean"]
            )
            return {
                "genes_present": genes_found,
                "gene_presence_absence": gene_presence_absence,
                # "blast_df": blast_df_unique
            }

    def __iter__(self):
        for analysis_alias in self.analysis_results:
            yield (analysis_alias)

    def __getitem__(self, analysis_alias):
        return {
            "results": self.analysis_results[analysis_alias],
            "output_files": self.analysis_output_files[analysis_alias],
        }


### Parse a fasta file, return list with headers matching what would appear as qseqid or sseqid in blast
def get_names_from_fasta(fasta_file):
    with open(fasta_file) as f:
        sequence_names = {}
        for line in f:
            if line[0] == ">":
                line = line.rstrip("\n")[1:]
                sequence_names[line.split()[0]] = line
    return sequence_names


class analysis_manager(sample_manager.input_manager):

    def __init__(self, input_config, analysis_settings_config):
        self.analysis_settings_config: dict = analysis_settings_config
        if not "output_folder" in input_config:
            input_config["output_folder"] = "./"
        self.base_output_folder = os.path.abspath(input_config["output_folder"])
        if not os.path.exists(self.base_output_folder):
            os.makedirs(self.base_output_folder)
        super().__init__(input_config)

    ### Initialize analysis_utility and add class instance to list in analysis_manager.samples

    def init_sample(self, attributes):
        if "output_folder" in attributes:
            output_folder = os.path.abspath(attributes["output_folder"])
        elif "sample_name" in attributes:
            output_folder = os.path.join(
                self.base_output_folder, attributes["sample_name"]
            )
        else:
            output_folder = self.base_output_folder
        sample = analysis_utility(
            attributes,
            self.base_input_folder,
            output_folder,
            self.analysis_settings_config,
        )
        return sample

    def write_to_tsv(self, include_metadata=True):
        if self.samples:
            output_file = os.path.join(self.base_output_folder, "results_summary.tsv")
            print_dicts = []
            for sample in self.samples:
                if include_metadata:
                    print_dict = sample.metadata.copy()
                    if "Illumina_read_files" in print_dict and isinstance(
                        print_dict["Illumina_read_files"], list
                    ):
                        print_dict["Illumina_read_files"] = ",".join(
                            print_dict["Illumina_read_files"]
                        )
                    print_dict = {
                        "sample_name": print_dict.pop("sample_name"),
                        **print_dict,
                    }  # Move sample_name to first column before printing for readability
                else:
                    print_dict = {"sample_name": sample.sample_name}
                for analysis_alias in sample.analysis_results:
                    print_dict.update(sample.analysis_results[analysis_alias])
                print_dicts.append(print_dict.copy())
            try:
                with open(output_file, "w") as o:
                    dict_writer = csv.DictWriter(
                        o, print_dicts[0].keys(), delimiter="\t"
                    )
                    dict_writer.writeheader()
                    dict_writer.writerows(print_dicts)
                o.close()
                print(f"Analysis summary written to {output_file}")
            except Exception as e:
                print(f"ERROR while writing analysis summary to {output_file}: {e}")

    def setup_log_config(self, log_file, log_level="INFO"):
        logger = logging.getLogger()
        logging.basicConfig(
            level=log_level,
            # filename=str(log_file),
            encoding="utf-8",
            filemode="w",
            format="{asctime} - {levelname} - {message}",
            style="{",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        return logger


def print_analysis_options(analysis_config):
    print("The following analyses are available:\n")
    analyses_to_run = analysis_config["analyses_to_run"]
    if isinstance(analyses_to_run, dict):
        for analysis, description in analyses_to_run.items():
            print(f"{analysis}: {description}")
            try:
                output_files_print = "\n    - ".join(
                    analysis_config[analysis]["files_to_clean"]
                )
                print(f" output_files:\n    - {output_files_print}\n")
            except:
                print("\n")
    elif isinstance(analyses_to_run, list):
        for analysis in analyses_to_run:
            print(f"{analysis}: {description}")
            try:
                output_files_print = "\n    - ".join(
                    analysis_config[analysis]["files_to_clean"]
                )
                print(f" output_files:\n    - {output_files_print}\n")
            except:
                print("\n")
    else:
        print("No analyses found in provided config")


def update_cleanup_config(analysis_config, files_to_keep_string):
    files_to_keep_list = files_to_keep_string.split(" ")
    for element in files_to_keep_list:
        element_split = element.split(":")
        if element_split[0] in analysis_config:
            if len(element_split) == 1:
                analysis_config[element]["files_to_clean"] = []
            else:
                analysis_config[element_split[0]]["files_to_clean"] = list(
                    set(analysis_config[element_split[0]]["files_to_clean"])
                    - set(element_split[1].split(","))
                )
    return analysis_config


def update_cli_input_config(
    input_config,
    analysis_config,
    samplesheet,
    input_folder,
    assembly_file,
    Illumina_read_files,
    Nanopore_read_file,
    output_folder,
    load_from_samplesheet,
    analyses_to_run,
    keep_files,
):
    if samplesheet is not None:
        input_config["samplesheet"] = samplesheet
        if input_folder is None:
            load_from_samplesheet = True
    if assembly_file is not None:
        input_config["assembly_file"] = assembly_file
    if Illumina_read_files is not None:
        input_config["Illumina_read_files"] = Illumina_read_files
    if Nanopore_read_file is not None:
        input_config["Nanopore_read_file"] = Nanopore_read_file
    if input_folder is not None:
        input_config["input_folder"] = input_folder
        if not input_config["load_from_samplesheet"]:
            input_config["load_from_folder"] = True
    if output_folder is not None:
        input_config["output_folder"] = output_folder
    if not analyses_to_run == "all":
        analysis_config["analyses_to_run"] = list(
            set(analysis_config["analyses_to_run"]).intersection(
                analyses_to_run.split(",")
            )
        )
    if keep_files is not None:
        analysis_config = update_cleanup_config(analysis_config, keep_files)
    input_config["load_from_samplesheet"] = load_from_samplesheet
    return (input_config, analysis_config)

# %% ../nbs/02_analysis_utility.ipynb 7
def unit_test_single():
    config = core.get_config()
    example_sample = analysis_utility(
        {
            "sample_name": "GAS-2022-1029",
            "assembly_file": "examples/GAS-2022-1029.fasta",
            "Illumina_read_files": [
                "examples/GAS-2022-1029_S42_L555_R1_001.fastq.gz",
                "examples/GAS-2022-1029_S42_L555_R2_001.fastq.gz",
            ],
        },
        input_folder=False,
        output_folder="output/",
        analysis_config=config["analysis_settings"]["Spyogenes"],
    )
    assert example_sample.sample_name == "GAS-2022-1029"
    assert len(example_sample.Illumina_read_files) == 2
    assert not example_sample.Nanopore_read_file


def unit_test_single_2():
    config = core.get_config()
    example_sample = analysis_utility(
        {
            "sample_name": "GAS-2024-0773",
            "assembly_file": "GAS-2024-0773.fasta",
            "Illumina_read_files": [
                "GAS-2024-0773_S35_L555_R1_001.fastq.gz",
                "GAS-2024-0773_S35_L555_R2_001.fastq.gz",
            ],
        },
        input_folder="examples",
        output_folder="output/",
        analysis_config=config["analysis_settings"]["Spyogenes"],
    )
    assert example_sample.sample_name == "GAS-2024-0773"
    assert len(example_sample.Illumina_read_files) == 2
    assert not example_sample.Nanopore_read_file


def unit_test_single_3():
    config = core.get_config()
    example_sample = analysis_utility(
        {
            "sample_name": "GAS-2022-1029",
            "assembly_file": "examples/GAS-2022-1029.fasta",
            "Illumina_read_files": [
                "examples/GAS-2022-1029_S42_L555_R1_001.fastq.gz",
                "examples/GAS-2022-1029_S42_L555_R2_001.fastq.gz",
            ],
            "samplesheet": "examples/samplesheet.tsv",
        },
        input_folder=False,
        output_folder="output/",
        analysis_config=config["analysis_settings"]["Spyogenes"],
    )
    assert example_sample.sample_name == "GAS-2022-1029"
    assert len(example_sample.Illumina_read_files) == 2
    assert not example_sample.Nanopore_read_file
    print(example_sample.metadata)


def unit_test_from_folder():
    config = core.get_config()
    input_config = config["input_manager"]
    input_config["load_from_folder"] = True
    input_config["input_folder"] = "examples/"
    input_config["output_folder"] = "output_from_folder/"
    input_config["analysis_config"] = config["analysis_settings"]["Spyogenes"]
    test = analysis_manager(input_config, config["analysis_settings"]["Spyogenes"])


def unit_test_from_samplesheet():
    config = core.get_config()
    input_config = config["input_manager"]
    input_config["load_from_samplesheet"] = True
    input_config["samplesheet"] = "examples/samplesheet.tsv"
    input_config["output_folder"] = "output_from_samplesheet/"
    input_config["analysis_config"] = config["analysis_settings"]["Spyogenes"]
    test = analysis_manager(input_config, config["analysis_settings"]["Spyogenes"])
    print(test.__dict__)
    for x in test:
        print(x.__dict__)
