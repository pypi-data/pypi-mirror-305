"""Core physical operators."""

import abc
import collections
from copy import deepcopy
from itertools import chain
import json
import logging
from operator import itemgetter
from pprint import pprint
import warnings
from deriva.core import ermrest_model as _em
from ..optimizer import symbols
from ..catalog.stubs import ModelStub
from .. import mmo

logger = logging.getLogger(__name__)

# placeholders for table name
__tname_placeholder__ = '{__table_name__}'
__sname_placeholder__ = '{__schema_name__}'

def _make_constraint_name(tname, *cnames, suffix=''):
    """Returns a constraint name from the given components."""
    constraint_name = f'{tname}_' + '_'.join(cnames)
    return constraint_name[:63 - len(suffix)] + f'_{suffix}'  # max supported length


#
# Physical operator (abstract) base class definition
#

class PhysicalOperator:
    """Abstract base class for the physical operators.

    A physical operator has two primary purposes:
      1. it should determine the table definition (i.e., relation schema) of the computed relation; and
      2. it must compute the relation using an iterator pattern, which should efficiently yield its rows.
    """
    def __init__(self):
        super(PhysicalOperator, self).__init__()

    @property
    def description(self):
        """Describes the computed relation (i.e., its relation schema).
        """
        # This method will return the instance's `_description` if it has one, if not it will return this instance's
        # `_child.description` if child is defined, else will return `None`.
        if hasattr(self, '_description'):
            return self._description
        elif hasattr(self, '_child') and hasattr(self._child, 'description'):
            return self._child.description
        else:
            return None

    @abc.abstractmethod
    def __iter__(self):
        """Returns a generator function. Must be implemented by subclasses."""

    @classmethod
    def _rename_row_attributes(cls, row, renames, always_copy=False):
        """Renames the attributes in the input `row` according to the `renames` mapping.

        :param row: an input row as a dictionary
        :param renames: a mapping of new to old attribute names in the form `{ new_name: old_name [, ...] }`
        :param always_copy: if True, always returns a copy of the input row rather than the original row, even when no columns are renamed
        :return: output row with columns renamed
        """
        if not renames:
            if always_copy:
                return row.copy()
            else:
                return row

        new_row = row.copy()
        for new_name, old_name in renames.items():
            new_row[new_name] = row[old_name]
            if old_name in new_row:
                del new_row[old_name]
        return new_row


#
# Supplementary operator definitions: buffer, metadata, temp var reference
#

class BufferedOperator (PhysicalOperator):
    """Buffers the tuples generated by an arbitrary child operator."""
    def __init__(self, child):
        super(BufferedOperator, self).__init__()
        assert isinstance(child, PhysicalOperator)
        self._child = child
        self._buffer = collections.deque()

    def __iter__(self):
        # This is not intended to be re-entrant, but could be made so if needed
        if self._buffer:
            for item in self._buffer:
                yield item
        else:
            for item in self._child:
                self._buffer.append(item)
                yield item


class Metadata (PhysicalOperator):
    """Metadata pass through operator."""
    def __init__(self, description):
        super(Metadata, self).__init__()
        self._description = description

    def __iter__(self):
        raise NotImplementedError('The "Metadata" operator cannot be iterated.')


class TempVarRef (PhysicalOperator):
    """References a temporary variable (i.e., computed relation)."""
    def __init__(self, computed_relation):
        super(TempVarRef, self).__init__()
        assert computed_relation is not None and hasattr(computed_relation, 'fetch')
        self._description = computed_relation.prejson()
        self._computed_relation = computed_relation

    def __iter__(self):
        return iter(self._computed_relation.fetch())


#
# Mutation primitive operators: assign, create, alter, drop
#

class Assign (PhysicalOperator):
    """Assign operator names the relation and passes through the child iterator."""
    def __init__(self, child, schema_name, table_name):
        super(Assign, self).__init__()
        assert isinstance(child, PhysicalOperator)
        self._child = child
        self._description = deepcopy(child.description)
        self._description['schema_name'] = schema_name
        self._description['table_name'] = table_name
        # finalize f/key names
        model_stub = ModelStub.for_table(self._description)
        for constraint_type in ['keys', 'foreign_keys']:
            for cdef in self._description[constraint_type]:
                self._finalize_constraint_name(schema_name, table_name, cdef, model_stub)
        # clear visible-fkeys, if any, as they cannot be valid for a newly defined table
        if 'annotations' in self._description and _em.tag.visible_foreign_keys in self._description['annotations']:
            self._description['annotations'][_em.tag.visible_foreign_keys] = {'*': []}

    def __iter__(self):
        return iter(self._child)

    @property
    def child(self):
        return self._child

    def _finalize_constraint_name(self, schema_name, table_name, constraint_def, model):
        """Finalizes constraint name in definition and in model annotations."""
        if constraint_def['names']:
            computed_name = constraint_def['names'][0]
            final_name = [schema_name, computed_name[1].replace(__tname_placeholder__, table_name)]
            constraint_def['names'] = [final_name]
            mmo.replace(model, computed_name, final_name)
            # todo: finalize acl bindings


class Create (Assign):
    """Create operator."""
    def __init__(self, child, schema_name, table_name):
        super(Create, self).__init__(child, schema_name, table_name)


class Alter (Assign):
    """Alter operator takes the 'projection' which will define the mutation on the relation."""
    def __init__(self, child, src_sname, src_tname, dst_sname, dst_tname, projection):
        super(Alter, self).__init__(child, dst_sname, dst_tname)
        self.src_sname, self.src_tname, self.dst_sname, self.dst_tname = src_sname, src_tname, dst_sname, dst_tname
        self.projection = projection


class Drop (Assign):
    """Drop operator."""
    def __init__(self, child, schema_name, table_name):
        super(Drop, self).__init__(child, schema_name, table_name)


#
# Basic primitive operators: select, project, rename, distinct, union
#


class Select (PhysicalOperator):
    """Select operator.
    """
    def __init__(self, child, formula):
        """Initializes the Select operator.

        :param child: input expression
        :param formula: select formula; basically a parsed where-clause
        """
        super(Select, self).__init__()
        assert isinstance(child, PhysicalOperator)
        assert isinstance(formula, symbols.Comparison) or \
               isinstance(formula, symbols.Conjunction) or isinstance(formula, symbols.Disjunction)
        logger.debug('Select "formula" => %s' % str(formula))
        self._child = child

        if isinstance(formula, symbols.Comparison):
            self._comparisons = [formula]
        else:
            self._comparisons = formula.comparisons

        if isinstance(formula, symbols.Disjunction):
            self._connector_fn = any
        else:
            self._connector_fn = all

    def __iter__(self):
        return filter(self._eval_formula, self._child)

    def _eval_comparison(self, row: dict, comparison: symbols.Comparison):
        """Evaluates a single comparison.
        """
        assert isinstance(comparison, symbols.Comparison)
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug('eval: %s %s %s' % (row[comparison.operand1], comparison.operator, comparison.operand2))
        return getattr(row[comparison.operand1], '__%s__' % comparison.operator)(comparison.operand2)

    def _eval_formula(self, row):
        """Evalutates the current row against the select operator's formula.
        """
        if not self._comparisons:  # if no comparisons than return tuple by default
            return True
        # otherwise, test that all comparisons match
        return self._connector_fn([self._eval_comparison(row, comparison) for comparison in self._comparisons])


class Project (PhysicalOperator):
    """Project operator.
    """
    def __init__(self, child, projection, **kwargs):
        """Instantiates the project operator from a child operation and a projection list of attributes.

        :param child: child operation
        :param projection: iterable collection of attributes or attribute-related symbols
        :param with_keys: project valid keys (default True)
        """
        super(Project, self).__init__()
        assert isinstance(child, PhysicalOperator)
        projection = projection or [symbols.AllAttributes()]
        assert hasattr(projection, '__iter__') and not isinstance(projection, str), '"projection" must be an iterable collection'
        self._child = child
        self._attributes = set()
        self._alias_to_cname = {}
        self._cname_to_alias = collections.defaultdict(list)
        with_keys = kwargs.get('with_keys', True)

        # Redefine the description of the child operator based on the projection
        table_def = self.description
        logger.debug("projecting from child relation: %s", table_def)
        schema_name = table_def.get('schema_name', __sname_placeholder__)
        removals = set()
        additions = []
        renamed_constraints = {}
        dropped_constraints = []
        dropped_columns = []
        fkey_defs = []

        #
        # Projection of column names: organize into complete set, alias, dropped, added, etc.
        #
        for item in projection:
            if isinstance(item, symbols.AllAttributes):
                logger.debug("projecting all attributes")
                self._attributes |= {col_def['name'] for col_def in table_def['column_definitions']}
            elif isinstance(item, str):
                logger.debug("projecting attribute by name: %s", item)
                self._attributes.add(item)
            elif isinstance(item, symbols.IntrospectionFunction):
                # Attributes may contain an introspection function. If so, call it on the table model object, and
                # combine its results with the rest of the given attributes list.
                logger.debug("projecting attributes returned by introspection function: %s", item)
                # introspect attributes, handle special case for 'RID'
                attrs = item.fn(table_def)
                if 'RID' in attrs:
                    attrs.remove('RID')
                    renamed_rid = table_def['table_name'] + '_RID'
                    self._alias_to_cname[renamed_rid] = 'RID'
                    self._cname_to_alias['RID'].append(renamed_rid)
                # add to projected attributes
                self._attributes |= set(attrs)
            elif isinstance(item, symbols.AttributeAlias):
                logger.debug("projecting an aliased attribute: %s", item)
                self._alias_to_cname[item.alias] = item.name
                self._cname_to_alias[item.name].append(item.alias)
            elif isinstance(item, symbols.AttributeDrop):
                logger.debug("projection with attribute drop: %s", item)
                removals.add(item.name)
            elif isinstance(item, symbols.AttributeAdd):
                logger.debug("projection with attribute add: %s", item)
                additions.append(json.loads(item.definition))
            else:
                raise ValueError("Unsupported projection type '{}'.".format(type(item).__name__))

        logger.debug("alias to cnames: %s", self._alias_to_cname)
        logger.debug("cname to aliases: %s", self._cname_to_alias)

        #
        # Column definitions based on projected attributes
        #
        projected_attrs = set()
        col_defs = []
        for col_def in table_def['column_definitions']:
            cname = col_def['name']
            if cname in self._attributes and cname not in removals and cname not in self._cname_to_alias:
                col_defs.append(col_def)
                projected_attrs.add(cname)
            elif cname in self._cname_to_alias:
                for alias in self._cname_to_alias[cname]:
                    # copy and rename column def
                    col_def = col_def.copy()
                    # if projecting a RID as a new column, fix its default and type
                    if col_def['name'] == 'RID':
                        col_def['type'] = _em.builtin_types.text.prejson()
                        col_def['default'] = None
                    col_def['name'] = alias
                    col_defs.append(col_def)
            else:
                dropped_columns.append(cname)
        col_defs.extend(additions)

        # Updated projection of attributes
        self._attributes = projected_attrs
        # set of all projected attributes, including those that will be renamed
        # will be used in the next steps to determine which keys and fkeys can be preserved
        all_projected_attributes = self._attributes | self._cname_to_alias.keys()

        #
        # Key projection: key definitions for which all key columns exist in this projection
        #
        key_defs = []
        for key_def in table_def['keys']:
            unique_columns = key_def['unique_columns']
            # include key if all unique columns are in the projection
            if with_keys and set(unique_columns) <= all_projected_attributes:
                key_def = key_def.copy()
                key_def['unique_columns'] = [self._cname_to_alias.get(cname, [cname])[0] for cname in unique_columns]
                # generate new name, remember old name(s)
                old_names = key_def['names']
                new_name = [schema_name, _make_constraint_name(__tname_placeholder__, *key_def['unique_columns'], suffix='key')]
                key_def['names'] = [new_name]
                key_defs.append(key_def)
                # record key new-old name, for renaming in annotations
                renamed_constraints[tuple(new_name)] = old_names
            else:
                # record key name, for pruning from annotations
                dropped_constraints.append(key_def['names'])

        #
        # Foreign Key projection: fkey definitions for which all fkey columns exist in this projection
        #
        for fkey_def in table_def['foreign_keys']:
            foreign_key_columns = [fkey_col['column_name'] for fkey_col in fkey_def['foreign_key_columns']]
            # include fkey if all fkey columns are in the projection
            if set(foreign_key_columns) <= all_projected_attributes:
                fkey_def = fkey_def.copy()
                revised_fkcols = [self._cname_to_alias.get(cname, [cname])[0] for cname in foreign_key_columns]
                fkey_def['foreign_key_columns'] = [
                    {'column_name': cname} for cname in revised_fkcols
                ]
                # generate new name, remember old name(s)
                old_names = fkey_def['names']
                new_name = [schema_name, _make_constraint_name(__tname_placeholder__, *revised_fkcols, suffix='fkey')]
                fkey_def['names'] = [new_name]
                fkey_defs.append(fkey_def)
                # record fkey new-old name, for renaming in annotations
                renamed_constraints[tuple(new_name)] = old_names
            else:
                # record fkey name, for pruning from annotations
                dropped_constraints.append(fkey_def['names'])

        #
        # Annotation projection: replace or prune constraints (keys/fkeys) and columns from annotations
        #
        annotations = deepcopy(table_def.get('annotations', {}))
        model_stub = ModelStub.for_table({
            'schema_name': schema_name,
            'table_name': __tname_placeholder__,
            'foreign_keys': fkey_defs,
            'annotations': annotations
        })

        # ...replace constraint names in model
        for new_name in renamed_constraints:
            for old_name in renamed_constraints[new_name]:
                mmo.replace(model_stub, old_name, list(new_name))

        # ...prune constraint names in model
        for old_names in dropped_constraints:
            for old_name in old_names:
                mmo.prune(model_stub, old_name)

        # ...prune columns in model
        for dropped_cname in dropped_columns:
            mmo.prune(model_stub, [schema_name, __tname_placeholder__, dropped_cname])

        # ...replace columns in model
        for alias in self._alias_to_cname:
            new_name = [schema_name, __tname_placeholder__, alias]
            old_name = [schema_name, __tname_placeholder__, self._alias_to_cname[alias]]
            mmo.replace(model_stub, old_name, new_name)

        #
        # Define the table
        #
        self._description = _em.Table.define(
            __tname_placeholder__,
            column_defs=col_defs,
            key_defs=key_defs,
            fkey_defs=fkey_defs,
            comment=table_def.get('comment', ''),
            acls=table_def.get('acls', {}),
            acl_bindings=table_def.get('acl_bindings', {}),  # TODO: projection of acl bindings
            annotations=annotations,
            provide_system=False
        )

    def __iter__(self):
        original_attributes = self._attributes | self._cname_to_alias.keys()
        getter = itemgetter(*original_attributes)
        for row in self._child:
            values = getter(row)
            values = values if isinstance(values, tuple) else (values,)
            assert len(original_attributes) == len(values)
            row = dict(zip(original_attributes, values))
            yield self._rename_row_attributes(row, self._alias_to_cname)


class Rename (Project):
    """Rename operator.
    """
    def __init__(self, child, renames):
        assert isinstance(child, PhysicalOperator)
        assert child.description
        assert isinstance(renames, tuple)
        assert all([isinstance(rename, symbols.AttributeAlias) for rename in renames])

        # compile list of renames, and handle >1 alias per original column
        rename_dict = collections.defaultdict(list)
        for rename in renames:
            rename_dict[rename.name].append(rename)

        # create a projection from original column definitions modulo the renamed columns
        projection = []
        for col in child.description['column_definitions']:
            if col['name'] in rename_dict:
                projection.extend(rename_dict[col['name']])
            else:
                projection.append(col['name'])

        # let the Project operator do the rest
        super(Rename, self).__init__(child, projection)


class HashDistinct (PhysicalOperator):
    """Distinct operator using in-memory hash data structure.
    """
    def __init__(self, child, attributes):
        """Initializes the operator.

        :param child: child expression
        :param attributes: subset of attributes in child to enforce set semantics on
        """
        super(HashDistinct, self).__init__()
        self._child = child
        self._distinct_on = attributes

    def __iter__(self):
        getter = itemgetter(*self._distinct_on)
        tuples = set()
        for row in self._child:
            tuple_ = getter(row)
            if tuple_ not in tuples:
                tuples.add(tuple_)
                yield row


class Union (PhysicalOperator):
    """Union operator.
    """
    def __init__(self, child, other):
        super(Union, self).__init__()
        assert isinstance(child, PhysicalOperator)
        assert isinstance(other, PhysicalOperator)
        other_columns = {c['name']: c for c in other.description['column_definitions']}
        if any([
            c['name'] not in other_columns or c != other_columns[c['name']]
            for c in child.description['column_definitions']
        ]):
            raise ValueError('Column definitions between tables in a union operation must match')
        self._child = child
        self._other = other

    def __iter__(self):
        return chain(self._child, self._other)


#
# Nest and unnest operators
#

class NestedLoopsSimilarityAggregation (Project):
    """Nested loops similarity aggregation operator.
    """
    def __init__(self, child, grouping, nesting, similarity_fn, grouping_fn):
        """Initializes the relation as a projection of 'grouping' with 'nesting' appended afterwards.
        """
        assert len(grouping) == 1, 'must specify one and only one grouping attribute'
        assert len(nesting) <= 1, 'only one nesting attribute allowed'
        super(NestedLoopsSimilarityAggregation, self).__init__(child, grouping)
        self._child = child
        self._grouping = grouping
        self._nesting = nesting
        self._similarity_fn = similarity_fn
        self._grouping_fn = grouping_fn
        self._description['column_definitions'] += [
            _em.Column.define(
                col['name'],
                _em.builtin_types[col['type']['typename'] + '[]'],
                comment=col['comment'],
                acls=col['acls'] if 'acls' in col else {},
                acl_bindings=col['acl_bindings'] if 'acl_bindings' in col else {},
                annotations=col['annotations'] if 'annotations' in col else {}
            ) for col in child.description['column_definitions'] if col['name'] in self._nesting
        ]

    def __iter__(self):
        # TODO: revisit the complexity of this algorithm... O(N) + O(N^2) + O(M)
        #       where N is the number of rows, and M is the number of groups

        # item getters
        key_getter = itemgetter(*self._grouping)
        nested_getter = itemgetter(*self._nesting) if self._nesting else None

        # keep a local cache of rows, b/c it will be iterated repeatedly
        rows = []
        # keep track of each key's membership in a group (i.e., grouping reverse index)
        member_of_list = []
        for row in self._child:
            rows.append(row)
            member_of_list.append({'key': key_getter(row), 'member_of': None})

        # accumulate groups
        groups = {}
        for row in rows:
            key1 = key_getter(row)
            for i, candidate in enumerate(member_of_list):
                if not candidate['member_of'] and self._similarity_fn(key1, candidate['key']) < 1.0:
                    # update the reverse index of groups
                    candidate['member_of'] = key1
                    # update the groups, by getting the corresponding i-th row and adding it to the group
                    if self._nesting:
                        group = groups.get(key1, set())
                        group.add(nested_getter(rows[i]))
                    else:
                        group = rows[i]
                    groups[key1] = group

        # yield groups
        for k, v in groups.items():
            # due to current limitation, assume a length of 1 for both parts of projection
            if self._nesting:
                yield {self._grouping[0]: k, self._nesting[0]: list(v)}
            else:
                # TODO: should probably yield only the grouping key and nothing else
                yield v


class Unnest (Project):
    """Unnest operator that allows user-defined function for custom unnesting.
    """
    def __init__(self, child, unnest_fn, attribute):
        """ Instantiates the operator as a projection of all but 'attribute' then adds 'attribute' back in.

        :param child: the child operator
        :param unnest_fn: unnesting function that takes a value and yields zero or more values
        :param attribute: name of the attribute to be used as input to the unnest_fn function
        """
        super(Unnest, self).__init__(child, (symbols.AllAttributes(), symbols.AttributeDrop(attribute)), with_keys=False)
        assert isinstance(child, PhysicalOperator)
        self._child = child
        self._unnest_fn = unnest_fn
        self._attribute = attribute

        # add 'attribute' back into column definitions
        for col_def in child._description['column_definitions']:
            if col_def['name'] == attribute:
                self._description['column_definitions'].append(deepcopy(col_def))
                break

    def __iter__(self):
        for row in self._child:
            for atom in self._unnest_fn(row[self._attribute]):
                # for each generated value produced by the unnest function, yield a copied row with the yielded atom
                copy = row.copy()
                copy[self._attribute] = atom
                yield copy


#
# Cross join and Similarity join operators
#


class CrossJoin (Project):
    """Cross-Join operator.
    """
    def __init__(self, left, right):
        """Instantiates the CrossJoin by projecting the left columns w/out keys and merging the right definition.
        """
        assert isinstance(left, PhysicalOperator) and isinstance(right, PhysicalOperator), 'child relations must be physical operator instances'
        self._left = left
        self._right = right
        self._left_renames, self._right_renames = dict(), dict()

        #
        # Project 'left' relation. Include all columns as is, except for RID renamed as 'left_RID'. Without keys.
        #
        left_projection = []
        for col_def in left.description['column_definitions']:
            cname = col_def['name']
            if cname == 'RID':
                alias = f'left_{cname}'
                left_projection.append(symbols.AttributeAlias(cname, alias))
                self._left_renames[alias] = cname
            else:
                left_projection.append(cname)

        super(CrossJoin, self).__init__(left, left_projection, with_keys=False)

        #
        # Project 'right' relation by renaming any conflicting columns "right_<original column name>"
        # ...yes, this has its limitations, but users can work around by renaming beforehand, when needed
        #
        existing_columns = {cdef['name'] for cdef in self.description['column_definitions']} | {'RID'}
        right_projection = []
        for col_def in right.description['column_definitions']:
            cname = col_def['name']
            if cname in existing_columns:
                alias = f'right_{cname}'
                right_projection.append(symbols.AttributeAlias(cname, alias))
                self._right_renames[alias] = cname
            else:
                right_projection.append(cname)

        right = Project(right, right_projection, with_keys=False)

        #
        # Merge the revised 'right' relation into the current definition based on the 'left' relatoin
        #

        # ...merge column definitions
        self._description['column_definitions'] += right.description['column_definitions']

        # ...merge fkey definitions
        self._description['foreign_keys'] += right.description['foreign_keys']

    def __iter__(self):
        for left_row in self._left:
            row = self._rename_row_attributes(left_row, self._left_renames, always_copy=True)
            for right_row in self._right:
                right_row = self._rename_row_attributes(right_row, self._right_renames)
                row.update(right_row)
                yield row


class NestedLoopsSimilarityJoin (CrossJoin):
    """Nested loops similarity join operator.
    """
    def __init__(self, left, right, condition):
        super(NestedLoopsSimilarityJoin, self).__init__(left, right)
        self._condition = condition
        assert isinstance(self._condition, symbols.Similar), "only similarity is supported in the comparison, currently"
        self._target = condition.attribute
        self._domain = condition.domain
        self._synonyms = condition.synonyms
        self._similarity_fn = condition.similarity_fn

    def __iter__(self):
        # TODO use grouping function to improve algorithm by comparing rows within sub-groups only
        right_rows = list(self._right)  # cache a copy of the right rows
        for left_row in self._left:
            target = left_row[self._target]
            best_match_score = 1.0
            best_match_row = None

            # look for the best match in the right_rows
            for right_row in right_rows:
                # compile list of domain name and synonyms
                synonyms = right_row[self._synonyms] if right_row[self._synonyms] is not None else []
                domain_and_synonyms = [right_row[self._domain]] + synonyms
                # test similarity of domain and synonyms
                for term in domain_and_synonyms:
                    similarity = self._similarity_fn(target, term)
                    if similarity < best_match_score:
                        best_match_score = similarity
                        best_match_row = right_row
                        if similarity == 0.0:
                            # found best possible match, stop comparing
                            break
                if best_match_score == 0.0:
                    # found best possible match, stop comparing
                    break

            if best_match_row:
                # only yield a row if a near match was satisfied
                row = self._rename_row_attributes(left_row, self._left_renames, always_copy=True)
                # join with best match
                row.update(
                    self._rename_row_attributes(best_match_row, self._right_renames, always_copy=True))
                yield row


#
# Integrity constraint modification operators: add key, add foreign key, ...
#

class IntegrityConstraintModificationOperator (PhysicalOperator):
    """Base class for integrity constraint modification operator (ICMO).
    """
    def __init__(self, child):
        super(IntegrityConstraintModificationOperator, self).__init__()
        self._child = child

    def __iter__(self):
        return iter(self._child)


class AddKey (IntegrityConstraintModificationOperator):
    """Add Key constraint operator.
    """
    def __init__(self, child, unique_columns):
        """Initializes the AddKey operator.

        :param child: input expression
        :param unique_columns: collection for the unique columns of the key
        """
        super(AddKey, self).__init__(child)
        assert isinstance(child, PhysicalOperator)
        if unique_columns == symbols.AllAttributes:
            # convert to all column names
            unique_columns = [cdoc['name'] for cdoc in child.description.get('column_definitions', [])]
        else:
            unique_columns = list(unique_columns) if isinstance(unique_columns, tuple) else unique_columns
        assert isinstance(unique_columns, list) and isinstance(next(iter(unique_columns)), str), '"unique_columns" must be a list of column names'
        logger.debug('unique_columns: %s' % str(unique_columns))
        self._child = child
        self._description = deepcopy(child.description)
        # add key definition to table description
        key_name = [self._description.get('schema_name', __sname_placeholder__), _make_constraint_name(__tname_placeholder__, *unique_columns, suffix='key')]
        self._description['keys'].append(
            _em.Key.define(unique_columns, constraint_names=[key_name])
        )
        # replace unique columns with key name in the default visible-columns
        vizcols = self._description.get('annotations', {}).get(_em.tag.visible_columns, {}).get('*')
        if isinstance(vizcols, list):
            vizcols = [item for item in vizcols if item not in unique_columns]
            vizcols.append(key_name)
            self._description['annotations'][_em.tag.visible_columns]['*'] = vizcols


class DropConstraint (IntegrityConstraintModificationOperator):
    """DropConstraint constraint operator.
    """

    KEYS = 'keys'
    FOREIGN_KEYS = 'foreign_keys'

    def __init__(self, child, constraint_name, constraint_type):
        """Initializes the DropConstraint operator.

        If no constraint name is given, all constraints of this type will be removed.

        :param child: input expression
        :param constraint_name: unqualified constraint name (or AllConstraints)
        :param constraint_type: type of constraint (use constants to specify)
        """
        super(DropConstraint, self).__init__(child)
        assert isinstance(child, PhysicalOperator)
        logger.debug('dropping constraint name: %s' % constraint_name)
        self._description = deepcopy(child.description)
        if constraint_name == symbols.AllConstraints:
            # clear all constraints of this type
            self._description[constraint_type] = []
        else:
            # include all but the named constraint(s)
            assert isinstance(constraint_name, str)
            self._description[constraint_type] = [
                c for c in self._description.get(constraint_type, []) if not DropConstraint._has_name(c, constraint_name)
            ]
        # todo: foreach dropped constraint, prune name from the model

    @classmethod
    def _has_name(cls, constraint, constraint_name):
        names = constraint.get('names', [])
        for name in names:
            if len(name) == 2 and name[1] == constraint_name:
                return True
        return False


class AddForeignKey (IntegrityConstraintModificationOperator):
    """Add Foreign Key constraint operator.
    """
    def __init__(self, left, right, referenced_columns, foreign_key_columns=None):
        """Initializes the AddForeignKey operator.

        If not 'foreign_key_columns' given, then the 'child' relation should have a set of columns matching
        referenced_columns.

        :param left: input expression for the referring relation
        :param right: input expression for the primary key table
        :param referenced_columns: collection of the referenced primary key column names, or the introspection function
        :param foreign_key_columns: collection of the foreign key column names (optional)
        """
        super(AddForeignKey, self).__init__(left)
        assert isinstance(left, PhysicalOperator)
        referenced_columns = list(referenced_columns) if isinstance(referenced_columns, tuple) else referenced_columns
        assert isinstance(referenced_columns, list), '"referenced_columns" must be a list'
        assert referenced_columns, '"referenced_columns" contain at least one column name or function'
        logger.debug('referenced_columns: %s' % referenced_columns)
        self._child = left
        self._description = deepcopy(left.description)

        # pk table may be a table object or a physical operator
        pk_table_def = right.prejson() if hasattr(right, 'prejson') else right.description
        if pk_table_def['table_name'] == __tname_placeholder__:
            warnings.warn('Introspecting a key on a computed relation is not recommended')

        # introspect referenced columns, if needed
        if isinstance(referenced_columns[0], symbols.IntrospectionFunction):
            key_introspection_fn = referenced_columns[0].fn
            referenced_columns = key_introspection_fn(pk_table_def)

        logger.debug('pk columns: %s' % referenced_columns)

        # define foreign key columns based on referenced pk columns, if needed
        if not foreign_key_columns:
            foreign_key_columns = [cname if cname != 'RID' else pk_table_def['table_name']+'_RID' for cname in referenced_columns]

        logger.debug('fk columns: %s' % foreign_key_columns)

        # define and append fkey
        fkey_name = [self._description.get('schema_name', __sname_placeholder__), _make_constraint_name(__tname_placeholder__, *foreign_key_columns, suffix='fkey')]
        self._description['foreign_keys'].append(
            _em.ForeignKey.define(
                foreign_key_columns,
                pk_table_def['schema_name'],
                pk_table_def['table_name'],
                referenced_columns,
                on_update='CASCADE',
                constraint_names=[fkey_name]
            )
        )

        # add fkey to default visible-columns
        vizcols = self._description.get('annotations', {}).get(_em.tag.visible_columns, {}).get('*')
        if isinstance(vizcols, list):
            vizcols.append(fkey_name)
